{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380e5471",
   "metadata": {},
   "source": [
    "# Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical Image Super-Resolution\n",
    "\n",
    "## What is Fast-DDPM?\n",
    "\n",
    "**Traditional DDPM (Denoising Diffusion Probabilistic Models):**\n",
    "- Forward process: Add Gaussian noise to images over 1000 timesteps\n",
    "- Reverse process: Learn to denoise images step-by-step\n",
    "- Problem: Very slow - requires 1000 denoising steps at inference\n",
    "\n",
    "**Fast-DDPM (Our Approach):**\n",
    "- **Key Innovation:** Use only 10 timesteps instead of 1000\n",
    "- **How:** Skip intermediate steps using accelerated sampling schedule (uniform or non-uniform)\n",
    "- **Result:** \n",
    "  - Training time: **0.2x of DDPM** (5x faster)\n",
    "  - Sampling time: **0.01x of DDPM** (100x faster!)\n",
    "  - Quality: Same or better than DDPM\n",
    "\n",
    "## How It Works\n",
    "\n",
    "```\n",
    "Standard DDPM (1000 steps):\n",
    "t=0 → t=1 → t=2 → ... → t=999 → t=1000 (all steps)\n",
    "\n",
    "Fast-DDPM (10 steps):\n",
    "t=0 → t=100 → t=200 → ... → t=900 → t=1000 (skip intermediate steps)\n",
    "      ↓\n",
    "      Jump to key timesteps only\n",
    "```\n",
    "\n",
    "## For Your Medical Image Task\n",
    "\n",
    "- **Input:** 3 consecutive slices [i, i+1, i+2]\n",
    "- **Task:** Generate the middle slice (i+1) only\n",
    "- **Advantage:** \n",
    "  - Simpler, faster training\n",
    "  - Middle slice prediction (most stable)\n",
    "  - Probabilistic approach with uncertainty\n",
    "  - Can ensemble multiple predictions for better quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5220adc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA B200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fe07e",
   "metadata": {},
   "source": [
    "## Noise Schedule: Pre-compute 1000 timesteps, use only 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6145954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps: [  0 111 222 333 444 555 666 777 888 999]\n"
     ]
    }
   ],
   "source": [
    "class DDPMScheduler:\n",
    "    def __init__(self, num_timesteps=1000, num_inference_steps=10, scheduler_type='uniform'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        \n",
    "        betas = np.linspace(0.0001, 0.02, num_timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas)\n",
    "        \n",
    "        self.betas = torch.from_numpy(betas).float()\n",
    "        self.alphas = torch.from_numpy(alphas).float()\n",
    "        self.alphas_cumprod = torch.from_numpy(alphas_cumprod).float()\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n",
    "        \n",
    "        if scheduler_type == 'uniform':\n",
    "            self.timesteps = np.linspace(0, num_timesteps - 1, num_inference_steps).astype(int)\n",
    "        else:\n",
    "            self.timesteps = np.ceil(np.linspace(0, num_timesteps - 1, num_inference_steps) ** 1.1).astype(int)\n",
    "        \n",
    "        self.timesteps = torch.from_numpy(self.timesteps).long()\n",
    "    \n",
    "    def add_noise(self, x0, t, noise):\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha * x0 + sqrt_one_minus_alpha * noise\n",
    "\n",
    "scheduler = DDPMScheduler(num_timesteps=1000, num_inference_steps=10, scheduler_type='uniform')\n",
    "print(f\"Timesteps: {scheduler.timesteps.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61a444",
   "metadata": {},
   "source": [
    "## UNet with Time Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ee5498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 12,989,121\n"
     ]
    }
   ],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = t.float().unsqueeze(-1) / 1000.0\n",
    "        return self.fc(t)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Use adaptive num_groups to handle any channel size\n",
    "        num_groups_in = max(1, in_ch // 4)\n",
    "        num_groups_out = max(1, out_ch // 4)\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(num_groups_in, in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(num_groups_out, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_fc = nn.Linear(time_dim, out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        h = h + self.time_fc(t_emb)[:, :, None, None]\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class FastDDPMUNet(nn.Module):\n",
    "    def __init__(self, in_ch=2, out_ch=1, base_ch=64, time_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_emb = TimeEmbedding(time_dim)\n",
    "        self.init_conv = nn.Conv2d(in_ch, base_ch, 3, padding=1)\n",
    "        \n",
    "        self.enc1 = ResBlock(base_ch, base_ch * 2, time_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ResBlock(base_ch * 2, base_ch * 4, time_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ResBlock(base_ch * 4, base_ch * 8, time_dim)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.bottleneck = ResBlock(base_ch * 8, base_ch * 8, time_dim)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(base_ch * 8, base_ch * 4, 2, 2)\n",
    "        self.dec3 = ResBlock(base_ch * 8, base_ch * 4, time_dim)  # Input: cat(base_ch*4, base_ch*4) = base_ch*8\n",
    "        self.upconv2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, 2, 2)\n",
    "        self.dec2 = ResBlock(base_ch * 4, base_ch * 2, time_dim)  # Input: cat(base_ch*2, base_ch*2) = base_ch*4\n",
    "        self.upconv1 = nn.ConvTranspose2d(base_ch * 2, base_ch, 2, 2)\n",
    "        self.dec1 = ResBlock(base_ch * 2, base_ch, time_dim)      # Input: cat(base_ch, base_ch) = base_ch*2\n",
    "        \n",
    "        num_groups_final = max(1, base_ch // 4)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups_final, base_ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch, out_ch, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_emb(t)\n",
    "        \n",
    "        h = self.init_conv(x)\n",
    "        e1 = self.enc1(h, t_emb)\n",
    "        h = self.pool1(e1)\n",
    "        e2 = self.enc2(h, t_emb)\n",
    "        h = self.pool2(e2)\n",
    "        e3 = self.enc3(h, t_emb)\n",
    "        h = self.pool3(e3)\n",
    "        \n",
    "        h = self.bottleneck(h, t_emb)\n",
    "        \n",
    "        h = self.upconv3(h)\n",
    "        h = torch.cat([h, e3], dim=1)\n",
    "        h = self.dec3(h, t_emb)\n",
    "        h = self.upconv2(h)\n",
    "        h = torch.cat([h, e2], dim=1)\n",
    "        h = self.dec2(h, t_emb)\n",
    "        h = self.upconv1(h)\n",
    "        h = torch.cat([h, e1], dim=1)\n",
    "        h = self.dec1(h, t_emb)\n",
    "        \n",
    "        return self.final(h)\n",
    "\n",
    "model = FastDDPMUNet(in_ch=2, out_ch=1)\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b0baf",
   "metadata": {},
   "source": [
    "## Data Loading from ModelDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22977bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18269 | Val: 3221 | Test: 4560\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../src')\n",
    "from ModelDataGenerator_1 import build_dataloader\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "CHECKPOINT_DIR = '../models'\n",
    "\n",
    "train_loader = build_dataloader(split='train', batch_size=BATCH_SIZE, augment=True, num_workers=NUM_WORKERS)\n",
    "val_loader = build_dataloader(split='val', batch_size=BATCH_SIZE, augment=False, num_workers=NUM_WORKERS)\n",
    "test_loader = build_dataloader(split='test', batch_size=BATCH_SIZE, augment=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train: {len(train_loader)} | Val: {len(val_loader)} | Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbf745",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91699dac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [512] and input of shape [4, 768, 64, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m history = {\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m: []}\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     val_loss = validate()\n\u001b[32m     73\u001b[39m     history[\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m].append(epoch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m noise = torch.randn_like(target)\n\u001b[32m     26\u001b[39m x_noisy = scheduler_device.add_noise(target, t, noise)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m pred_noise = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m loss = criterion(pred_noise, noise)\n\u001b[32m     31\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mFastDDPMUNet.forward\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     82\u001b[39m h = \u001b[38;5;28mself\u001b[39m.upconv3(h)\n\u001b[32m     83\u001b[39m h = torch.cat([h, e3], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdec3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m h = \u001b[38;5;28mself\u001b[39m.upconv2(h)\n\u001b[32m     86\u001b[39m h = torch.cat([h, e2], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mResBlock.forward\u001b[39m\u001b[34m(self, x, t_emb)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t_emb):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     h = F.silu(h)\n\u001b[32m     32\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.conv1(h)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/modules/normalization.py:313\u001b[39m, in \u001b[36mGroupNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/functional.py:2960\u001b[39m, in \u001b[36mgroup_norm\u001b[39m\u001b[34m(input, num_groups, weight, bias, eps)\u001b[39m\n\u001b[32m   2953\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2954\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2955\u001b[39m     )\n\u001b[32m   2956\u001b[39m _verify_batch_size(\n\u001b[32m   2957\u001b[39m     [\u001b[38;5;28minput\u001b[39m.size(\u001b[32m0\u001b[39m) * \u001b[38;5;28minput\u001b[39m.size(\u001b[32m1\u001b[39m) // num_groups, num_groups]\n\u001b[32m   2958\u001b[39m     + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m.size()[\u001b[32m2\u001b[39m:])\n\u001b[32m   2959\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2961\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2962\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [512] and input of shape [4, 768, 64, 64]"
     ]
    }
   ],
   "source": [
    "model = FastDDPMUNet(in_ch=2, out_ch=1).to(DEVICE)\n",
    "scheduler_device = DDPMScheduler(num_timesteps=1000, num_inference_steps=10, scheduler_type='uniform')\n",
    "for key in ['betas', 'alphas', 'alphas_cumprod', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod']:\n",
    "    setattr(scheduler_device, key, getattr(scheduler_device, key).to(DEVICE))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for (pre, post), target in tqdm(train_loader, leave=False):\n",
    "        # Unpack: (pre, post) is a tuple, target is single tensor\n",
    "        pre = pre.to(DEVICE)      # (B, 1, H, W)\n",
    "        post = post.to(DEVICE)    # (B, 1, H, W)\n",
    "        target = target.to(DEVICE)  # (B, 1, H, W)\n",
    "        \n",
    "        x_input = torch.cat([pre, post], dim=1)  # (B, 2, H, W)\n",
    "        \n",
    "        batch_size = x_input.shape[0]\n",
    "        t_idx = torch.randint(0, len(scheduler_device.timesteps), (batch_size,))\n",
    "        t = scheduler_device.timesteps[t_idx].to(DEVICE)\n",
    "        \n",
    "        noise = torch.randn_like(target)\n",
    "        x_noisy = scheduler_device.add_noise(target, t, noise)\n",
    "        \n",
    "        pred_noise = model(x_input, t)\n",
    "        loss = criterion(pred_noise, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_loader)\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for (pre, post), target in tqdm(val_loader, leave=False):\n",
    "            # Unpack: (pre, post) is a tuple, target is single tensor\n",
    "            pre = pre.to(DEVICE)      # (B, 1, H, W)\n",
    "            post = post.to(DEVICE)    # (B, 1, H, W)\n",
    "            target = target.to(DEVICE)  # (B, 1, H, W)\n",
    "            \n",
    "            x_input = torch.cat([pre, post], dim=1)  # (B, 2, H, W)\n",
    "            \n",
    "            batch_size = x_input.shape[0]\n",
    "            t_idx = torch.randint(0, len(scheduler_device.timesteps), (batch_size,))\n",
    "            t = scheduler_device.timesteps[t_idx].to(DEVICE)\n",
    "            \n",
    "            noise = torch.randn_like(target)\n",
    "            x_noisy = scheduler_device.add_noise(target, t, noise)\n",
    "            \n",
    "            pred_noise = model(x_input, t)\n",
    "            loss = criterion(pred_noise, noise)\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(val_loader)\n",
    "\n",
    "best_loss = float('inf')\n",
    "history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch:2d}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\", end=\"\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/fastddpm_best.pt')\n",
    "        print(\" ✅\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "with open(f'{CHECKPOINT_DIR}/fastddpm_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\nBest Val Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54269a5b",
   "metadata": {},
   "source": [
    "## Sampling (Reverse Diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(pre, post, num_samples=1):\n",
    "    \"\"\"\n",
    "    Generate samples using the learned diffusion model\n",
    "    Args:\n",
    "        pre: Input slice i (B, 1, H, W)\n",
    "        post: Input slice i+4 (B, 1, H, W)\n",
    "        num_samples: Number of samples to generate\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_size = pre.shape[0]\n",
    "    \n",
    "    generated = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        x_t = torch.randn(batch_size, 1, pre.shape[2], pre.shape[3]).to(DEVICE)\n",
    "        x_input = torch.cat([pre.to(DEVICE), post.to(DEVICE)], dim=1)\n",
    "        \n",
    "        # Reverse diffusion process\n",
    "        for step_idx, t_idx in enumerate(reversed(range(len(scheduler_device.timesteps)))):\n",
    "            t = scheduler_device.timesteps[t_idx].to(DEVICE)\n",
    "            \n",
    "            # Predict noise\n",
    "            t_batch = t.unsqueeze(0).expand(batch_size)\n",
    "            pred_noise = model(x_input, t_batch)\n",
    "            \n",
    "            # Get alphas for this timestep and previous timestep\n",
    "            alpha_t = scheduler_device.alphas_cumprod[t.item()]\n",
    "            t_prev_idx = max(0, t_idx - 1)\n",
    "            alpha_prev = scheduler_device.alphas_cumprod[scheduler_device.timesteps[t_prev_idx].item()]\n",
    "            \n",
    "            # Compute posterior variance\n",
    "            posterior_var = (1.0 - alpha_prev) / (1.0 - alpha_t) * (1.0 - alpha_t / alpha_prev)\n",
    "            posterior_var = torch.clamp(posterior_var, min=1e-20)\n",
    "            \n",
    "            # Sample noise if not at final step\n",
    "            if t_idx > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x_t)\n",
    "            \n",
    "            # Reverse diffusion step\n",
    "            x_t = (1.0 / torch.sqrt(alpha_t)) * (x_t - (1.0 - alpha_t) / torch.sqrt(1.0 - alpha_t) * pred_noise) + torch.sqrt(posterior_var) * noise\n",
    "        \n",
    "        generated.append(x_t.cpu())\n",
    "    \n",
    "    return torch.stack(generated, dim=1)\n",
    "\n",
    "print(\"✅ Sampling function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cf4632",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab32079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/fastddpm_best.pt'))\n",
    "\n",
    "ssim_scores = []\n",
    "psnr_scores = []\n",
    "\n",
    "for (pre, post), target in tqdm(test_loader):\n",
    "    generated = sample(pre, post, num_samples=3)\n",
    "    pred = generated.mean(dim=1).squeeze().cpu().numpy()\n",
    "    gt = target.squeeze().cpu().numpy()\n",
    "    \n",
    "    for i in range(len(gt)):\n",
    "        gt_norm = (gt[i] - gt[i].min()) / (gt[i].max() - gt[i].min() + 1e-8)\n",
    "        pred_norm = (pred[i] - pred[i].min()) / (pred[i].max() - pred[i].min() + 1e-8)\n",
    "        \n",
    "        ssim_scores.append(ssim(gt_norm, pred_norm, data_range=1.0))\n",
    "        psnr_scores.append(psnr(gt_norm, pred_norm, data_range=1.0))\n",
    "\n",
    "print(f\"\\nSSIM: {np.mean(ssim_scores):.4f} ± {np.std(ssim_scores):.4f}\")\n",
    "print(f\"PSNR: {np.mean(psnr_scores):.2f} ± {np.std(psnr_scores):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e235bf6",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf60987",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['epoch'], history['train_loss'], 'o-', label='Train')\n",
    "plt.plot(history['epoch'], history['val_loss'], 's-', label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Fast-DDPM Training')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHECKPOINT_DIR}/fastddpm_training.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339a9e2",
   "metadata": {},
   "source": [
    "## Noise Schedule: Accelerated Timestep Selection\n",
    "\n",
    "Pre-compute all 1000 noise levels, but sample only 10 timesteps during training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6717c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMScheduler:\n",
    "    def __init__(self, num_timesteps=1000, num_inference_steps=10, scheduler_type='uniform'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        \n",
    "        # Pre-compute alphas for all 1000 timesteps\n",
    "        betas = np.linspace(0.0001, 0.02, num_timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas)\n",
    "        \n",
    "        self.register_buffer('betas', torch.from_numpy(betas).float())\n",
    "        self.register_buffer('alphas', torch.from_numpy(alphas).float())\n",
    "        self.register_buffer('alphas_cumprod', torch.from_numpy(alphas_cumprod).float())\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(torch.from_numpy(alphas_cumprod)).float())\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - torch.from_numpy(alphas_cumprod)).float())\n",
    "        \n",
    "        # Select timesteps: 10 instead of 1000\n",
    "        if scheduler_type == 'uniform':\n",
    "            self.timesteps = np.linspace(0, num_timesteps - 1, num_inference_steps).astype(int)\n",
    "        else:  # non-uniform\n",
    "            self.timesteps = np.ceil(np.linspace(0, num_timesteps - 1, num_inference_steps) ** 1.1).astype(int)\n",
    "        \n",
    "        self.timesteps = torch.from_numpy(self.timesteps).long()\n",
    "    \n",
    "    def register_buffer(self, name, tensor):\n",
    "        setattr(self, name, tensor)\n",
    "    \n",
    "    def add_noise(self, x0, t, noise):\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha * x0 + sqrt_one_minus_alpha * noise\n",
    "\n",
    "scheduler = DDPMScheduler(num_timesteps=1000, num_inference_steps=10, scheduler_type='uniform')\n",
    "print(f\"Selected timesteps: {scheduler.timesteps.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891efc6",
   "metadata": {},
   "source": [
    "## UNet with Time Embedding\n",
    "\n",
    "Lightweight UNet for conditioning on timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        # t is shape (batch_size,) - ensure it's 2D (batch_size, 1)\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)  # Convert scalar to (1,)\n",
    "        t_input = t.float().unsqueeze(-1) / 1000.0  # (batch_size, 1)\n",
    "        return self.fc(t_input)  # (batch_size, dim)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # CRITICAL FIX: Use adaptive num_groups (not hardcoded 32!)\n",
    "        num_groups_in = max(1, in_ch // 4)\n",
    "        num_groups_out = max(1, out_ch // 4)\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(num_groups_in, in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(num_groups_out, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_fc = nn.Linear(time_dim, out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        h = h + self.time_fc(t_emb)[:, :, None, None]\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class FastDDPMUNet(nn.Module):\n",
    "    def __init__(self, in_ch=2, out_ch=1, base_ch=64, time_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_emb = TimeEmbedding(time_dim)\n",
    "        self.init_conv = nn.Conv2d(in_ch, base_ch, 3, padding=1)\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ResBlock(base_ch, base_ch * 2, time_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ResBlock(base_ch * 2, base_ch * 4, time_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ResBlock(base_ch * 4, base_ch * 8, time_dim)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResBlock(base_ch * 8, base_ch * 8, time_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv3 = nn.ConvTranspose2d(base_ch * 8, base_ch * 4, 2, 2)\n",
    "        self.dec3 = ResBlock(base_ch * 8, base_ch * 4, time_dim)\n",
    "        self.upconv2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, 2, 2)\n",
    "        self.dec2 = ResBlock(base_ch * 4, base_ch * 2, time_dim)\n",
    "        self.upconv1 = nn.ConvTranspose2d(base_ch * 2, base_ch, 2, 2)\n",
    "        self.dec1 = ResBlock(base_ch * 2, base_ch, time_dim)\n",
    "        \n",
    "        num_groups_final = max(1, base_ch // 4)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups_final, base_ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch, out_ch, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_emb(t)\n",
    "        \n",
    "        h = self.init_conv(x)\n",
    "        e1 = self.enc1(h, t_emb)\n",
    "        h = self.pool1(e1)\n",
    "        e2 = self.enc2(h, t_emb)\n",
    "        h = self.pool2(e2)\n",
    "        e3 = self.enc3(h, t_emb)\n",
    "        h = self.pool3(e3)\n",
    "        \n",
    "        h = self.bottleneck(h, t_emb)\n",
    "        \n",
    "        h = self.upconv3(h)\n",
    "        h = torch.cat([h, e3], dim=1)\n",
    "        h = self.dec3(h, t_emb)\n",
    "        h = self.upconv2(h)\n",
    "        h = torch.cat([h, e2], dim=1)\n",
    "        h = self.dec2(h, t_emb)\n",
    "        h = self.upconv1(h)\n",
    "        h = torch.cat([h, e1], dim=1)\n",
    "        h = self.dec1(h, t_emb)\n",
    "        \n",
    "        return self.final(h)\n",
    "\n",
    "model = FastDDPMUNet(in_ch=2, out_ch=1).to(DEVICE)\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc4dd8",
   "metadata": {},
   "source": [
    "## Configuration & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "from ModelDataGenerator import build_dataloader\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "CHECKPOINT_DIR = '../models'\n",
    "\n",
    "train_loader = build_dataloader(split='train', batch_size=BATCH_SIZE, augment=True, num_workers=NUM_WORKERS)\n",
    "val_loader = build_dataloader(split='val', batch_size=BATCH_SIZE, augment=False, num_workers=NUM_WORKERS)\n",
    "test_loader = build_dataloader(split='test', batch_size=BATCH_SIZE, augment=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train: {len(train_loader)} | Val: {len(val_loader)} | Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d29b19",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastDDPMUNet(in_ch=2, out_ch=1).to(DEVICE)\n",
    "scheduler_device = DDPMScheduler(num_timesteps=1000, num_inference_steps=10, scheduler_type='uniform')\n",
    "for key in ['betas', 'alphas', 'alphas_cumprod', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod']:\n",
    "    setattr(scheduler_device, key, getattr(scheduler_device, key).to(DEVICE))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for (pre, post), target in tqdm(train_loader, leave=False):\n",
    "        pre, post, target = pre.to(DEVICE), post.to(DEVICE), target.to(DEVICE)\n",
    "        x_input = torch.cat([pre, post], dim=1)  # (B, 2, H, W)\n",
    "        \n",
    "        batch_size = x_input.shape[0]\n",
    "        t_idx = torch.randint(0, len(scheduler_device.timesteps), (batch_size,))\n",
    "        t = scheduler_device.timesteps[t_idx].to(DEVICE)\n",
    "        \n",
    "        noise = torch.randn_like(target)\n",
    "        x_noisy = scheduler_device.add_noise(target, t, noise)\n",
    "        \n",
    "        pred_noise = model(x_input, t)\n",
    "        loss = criterion(pred_noise, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_loader)\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for (pre, post), target in tqdm(val_loader, leave=False):\n",
    "            pre, post, target = pre.to(DEVICE), post.to(DEVICE), target.to(DEVICE)\n",
    "            x_input = torch.cat([pre, post], dim=1)\n",
    "            \n",
    "            batch_size = x_input.shape[0]\n",
    "            t_idx = torch.randint(0, len(scheduler_device.timesteps), (batch_size,))\n",
    "            t = scheduler_device.timesteps[t_idx].to(DEVICE)\n",
    "            \n",
    "            noise = torch.randn_like(target)\n",
    "            x_noisy = scheduler_device.add_noise(target, t, noise)\n",
    "            \n",
    "            pred_noise = model(x_input, t)\n",
    "            loss = criterion(pred_noise, noise)\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(val_loader)\n",
    "\n",
    "best_loss = float('inf')\n",
    "history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch:2d}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\", end=\"\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/fastddpm_best.pt')\n",
    "        print(\" ✅\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "with open(f'{CHECKPOINT_DIR}/fastddpm_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\nBest Val Loss: {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fb950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL: Redefine model with FIXED GroupNorm (adaptive num_groups)\n",
    "# ============================================================================\n",
    "# This ensures the latest model definition is used\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Clear old model definitions\n",
    "if 'FastDDPMUNet' in dir():\n",
    "    del FastDDPMUNet\n",
    "if 'ResBlock' in dir():\n",
    "    del ResBlock\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = t.float().unsqueeze(-1) / 1000.0\n",
    "        return self.fc(t)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # CRITICAL FIX: Use adaptive num_groups (not hardcoded 32!)\n",
    "        num_groups_in = max(1, in_ch // 4)\n",
    "        num_groups_out = max(1, out_ch // 4)\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(num_groups_in, in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(num_groups_out, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_fc = nn.Linear(time_dim, out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        h = h + self.time_fc(t_emb)[:, :, None, None]\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class FastDDPMUNet(nn.Module):\n",
    "    def __init__(self, in_ch=2, out_ch=1, base_ch=64, time_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_emb = TimeEmbedding(time_dim)\n",
    "        self.init_conv = nn.Conv2d(in_ch, base_ch, 3, padding=1)\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ResBlock(base_ch, base_ch * 2, time_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ResBlock(base_ch * 2, base_ch * 4, time_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ResBlock(base_ch * 4, base_ch * 8, time_dim)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResBlock(base_ch * 8, base_ch * 8, time_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv3 = nn.ConvTranspose2d(base_ch * 8, base_ch * 4, 2, 2)\n",
    "        self.dec3 = ResBlock(base_ch * 8, base_ch * 4, time_dim)\n",
    "        self.upconv2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, 2, 2)\n",
    "        self.dec2 = ResBlock(base_ch * 4, base_ch * 2, time_dim)\n",
    "        self.upconv1 = nn.ConvTranspose2d(base_ch * 2, base_ch, 2, 2)\n",
    "        self.dec1 = ResBlock(base_ch * 2, base_ch, time_dim)\n",
    "        \n",
    "        num_groups_final = max(1, base_ch // 4)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups_final, base_ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch, out_ch, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_emb(t)\n",
    "        \n",
    "        h = self.init_conv(x)\n",
    "        e1 = self.enc1(h, t_emb)\n",
    "        h = self.pool1(e1)\n",
    "        e2 = self.enc2(h, t_emb)\n",
    "        h = self.pool2(e2)\n",
    "        e3 = self.enc3(h, t_emb)\n",
    "        h = self.pool3(e3)\n",
    "        \n",
    "        h = self.bottleneck(h, t_emb)\n",
    "        \n",
    "        h = self.upconv3(h)\n",
    "        h = torch.cat([h, e3], dim=1)\n",
    "        h = self.dec3(h, t_emb)\n",
    "        h = self.upconv2(h)\n",
    "        h = torch.cat([h, e2], dim=1)\n",
    "        h = self.dec2(h, t_emb)\n",
    "        h = self.upconv1(h)\n",
    "        h = torch.cat([h, e1], dim=1)\n",
    "        h = self.dec1(h, t_emb)\n",
    "        \n",
    "        return self.final(h)\n",
    "\n",
    "# Create NEW model with fixed architecture\n",
    "print(\"✅ Creating FastDDPMUNet with FIXED adaptive GroupNorm...\")\n",
    "model = FastDDPMUNet(in_ch=2, out_ch=1).to(DEVICE)\n",
    "print(f\"✅ Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff6633",
   "metadata": {},
   "source": [
    "## Sampling (Fast-DDPM Reverse Process)\n",
    "\n",
    "Generate middle slice from (pre, post) using only 10 denoising steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(pre, post, num_samples=1):\n",
    "    model.eval()\n",
    "    batch_size = pre.shape[0]\n",
    "    \n",
    "    generated = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        x_t = torch.randn(batch_size, 1, pre.shape[2], pre.shape[3]).to(DEVICE)\n",
    "        \n",
    "        for step_idx, t in enumerate(reversed(scheduler_device.timesteps)):\n",
    "            t = t.to(DEVICE)\n",
    "            x_input = torch.cat([pre.to(DEVICE), post.to(DEVICE)], dim=1)\n",
    "            \n",
    "            pred_noise = model(x_input, t.unsqueeze(0).expand(batch_size))\n",
    "            \n",
    "            alpha = scheduler_device.alphas_cumprod[t]\n",
    "            alpha_prev = scheduler_device.alphas_cumprod[scheduler_device.timesteps[max(0, step_idx - 1)]]\n",
    "            \n",
    "            posterior_var = (1 - alpha_prev) / (1 - alpha) * (1 - alpha / alpha_prev)\n",
    "            posterior_var = torch.clamp(posterior_var, min=1e-20)\n",
    "            \n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x_t)\n",
    "            \n",
    "            x_t = (1 / torch.sqrt(alpha)) * (x_t - (1 - alpha) / torch.sqrt(1 - alpha) * pred_noise) + torch.sqrt(posterior_var) * noise\n",
    "        \n",
    "        generated.append(x_t.cpu())\n",
    "    \n",
    "    return torch.stack(generated, dim=1)  # (B, num_samples, 1, H, W)\n",
    "\n",
    "print(\"Sampling function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f31aa9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56415339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/fastddpm_best.pt'))\n",
    "\n",
    "ssim_scores = []\n",
    "psnr_scores = []\n",
    "\n",
    "for (pre, post), target in tqdm(test_loader):\n",
    "    generated = sample(pre, post, num_samples=3)\n",
    "    pred = generated.mean(dim=1).squeeze().cpu().numpy()\n",
    "    gt = target.squeeze().cpu().numpy()\n",
    "    \n",
    "    for i in range(len(gt)):\n",
    "        gt_norm = (gt[i] - gt[i].min()) / (gt[i].max() - gt[i].min() + 1e-8)\n",
    "        pred_norm = (pred[i] - pred[i].min()) / (pred[i].max() - pred[i].min() + 1e-8)\n",
    "        \n",
    "        ssim_scores.append(ssim(gt_norm, pred_norm, data_range=1.0))\n",
    "        psnr_scores.append(psnr(gt_norm, pred_norm, data_range=1.0))\n",
    "\n",
    "print(f\"\\nSSIM: {np.mean(ssim_scores):.4f} ± {np.std(ssim_scores):.4f}\")\n",
    "print(f\"PSNR: {np.mean(psnr_scores):.2f} ± {np.std(psnr_scores):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a307f4",
   "metadata": {},
   "source": [
    "## Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff98442",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['epoch'], history['train_loss'], 'o-', label='Train')\n",
    "plt.plot(history['epoch'], history['val_loss'], 's-', label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Fast-DDPM Training')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHECKPOINT_DIR}/fastddpm_training.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmia-2",
   "language": "python",
   "name": "dlmia-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
