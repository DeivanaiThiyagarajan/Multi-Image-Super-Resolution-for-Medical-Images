{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fd47eb",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e15e3a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA B200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe6cd8f",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ab68cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Pre-caching volumes into RAM for faster data loading...\n",
      "âœ… Cached 641 volumes in RAM\n",
      "ðŸ’¾ Pre-caching volumes into RAM for faster data loading...\n",
      "âœ… Cached 113 volumes in RAM\n",
      "ðŸ’¾ Pre-caching volumes into RAM for faster data loading...\n",
      "âœ… Cached 160 volumes in RAM\n",
      "âœ… Data loaded: Train=18269, Val=3221, Test=4560\n",
      "\n",
      "Batch shapes:\n",
      "  pre: torch.Size([4, 1, 256, 256])\n",
      "  post: torch.Size([4, 1, 256, 256])\n",
      "  target: torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Add src to path and import data generator\n",
    "sys.path.insert(0, '../src')\n",
    "from ModelDataGenerator_1 import build_dataloader\n",
    "\n",
    "# Configuration - OPTIMAL DDPM HYPERPARAMETERS\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 20\n",
    "LR = 2e-5  # 0.00002 - Optimal learning rate for DDPM\n",
    "BETA_START = 0.0001\n",
    "BETA_END = 0.02\n",
    "NUM_DIFFUSION_TIMESTEPS = 1000\n",
    "CHECKPOINT_DIR = '../models'\n",
    "results_dir = '../results'\n",
    "\n",
    "# Build dataloaders\n",
    "train_loader = build_dataloader(split='train', batch_size=BATCH_SIZE, augment=True, num_workers=NUM_WORKERS)\n",
    "val_loader = build_dataloader(split='val', batch_size=BATCH_SIZE, augment=False, num_workers=NUM_WORKERS)\n",
    "test_loader = build_dataloader(split='test', batch_size=BATCH_SIZE, augment=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"âœ… Data loaded: Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)}\")\n",
    "\n",
    "# Check one batch\n",
    "(pre_sample, post_sample), target_sample = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  pre: {pre_sample.shape}\")\n",
    "print(f\"  post: {post_sample.shape}\")\n",
    "print(f\"  target: {target_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d0d47",
   "metadata": {},
   "source": [
    "## 3. Noise Schedule (DDPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d95f84a-4ee4-4b49-919f-77eae8adbaee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50abdff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scheduler created with optimal config\n",
      "   Beta range: [0.0001, 0.02]\n",
      "   Timesteps: 1000\n",
      "   Inference steps: 10\n"
     ]
    }
   ],
   "source": [
    "class DDPMScheduler:\n",
    "    def __init__(self, num_timesteps=1000, num_inference_steps=10, scheduler_type='non-uniform', beta_schedule='linear'):\n",
    "        \n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.scheduler_type = scheduler_type\n",
    "        \n",
    "        # Pre-compute noise schedule with optimal beta range [0.0001, 0.02]\n",
    "        if beta_schedule == 'cosine':\n",
    "            # Cosine annealing schedule\n",
    "            s = 0.008\n",
    "            steps = torch.arange(0, num_timesteps + 1, dtype=torch.float32)\n",
    "            alphas_cumprod = torch.cos(((steps / num_timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "            betas = 1.0 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "            betas = torch.clamp(betas, 0.0001, 0.9999)\n",
    "        else:\n",
    "            # Linear schedule (optimal for DDPM) with range [0.0001, 0.02]\n",
    "            betas = torch.linspace(0.0001, 0.02, num_timesteps)\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod))\n",
    "        \n",
    "        # Select timesteps based on strategy\n",
    "        if scheduler_type == 'uniform':\n",
    "            # Uniform spacing: every skip-th timestep\n",
    "            skip = num_timesteps // num_inference_steps\n",
    "            self.timesteps = torch.arange(0, num_timesteps, skip).long()[:num_inference_steps]\n",
    "        \n",
    "        elif scheduler_type == 'non-uniform':\n",
    "            # Non-uniform: concentrate in noisy region\n",
    "            if num_inference_steps == 50:\n",
    "                # For 50 steps: more granular coverage\n",
    "                self.timesteps = torch.linspace(0, num_timesteps - 1, num_inference_steps).long()\n",
    "            else:\n",
    "                # Adaptive non-uniform for other step counts\n",
    "                num_stage1 = int(num_inference_steps * 0.4)\n",
    "                num_stage2 = int(num_inference_steps * 0.6)\n",
    "                \n",
    "                if num_stage1 > 0:\n",
    "                    stage1 = torch.linspace(0, 699, num_stage1 + 1)[:-1].ceil().long()\n",
    "                else:\n",
    "                    stage1 = torch.tensor([]).long()\n",
    "                \n",
    "                stage2 = torch.linspace(699, 999, num_stage2 + 1)[:-1].ceil().long()\n",
    "                self.timesteps = torch.cat([stage1, stage2])\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scheduler_type: {scheduler_type}\")\n",
    "    \n",
    "    def register_buffer(self, name, tensor):\n",
    "        setattr(self, name, tensor)\n",
    "    \n",
    "    def add_noise(self, x0, t, noise):\n",
    "        \"\"\"Forward process: x_t = sqrt(alpha_t)*x0 + sqrt(1-alpha_t)*eps\"\"\"\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha * x0 + sqrt_one_minus_alpha * noise\n",
    "\n",
    "# Create scheduler with optimal configuration\n",
    "scheduler = DDPMScheduler(num_timesteps=NUM_DIFFUSION_TIMESTEPS, num_inference_steps=10, scheduler_type='non-uniform', beta_schedule='linear')\n",
    "print(f\"âœ… Scheduler created with optimal config\")\n",
    "print(f\"   Beta range: [0.0001, 0.02]\")\n",
    "print(f\"   Timesteps: {NUM_DIFFUSION_TIMESTEPS}\")\n",
    "print(f\"   Inference steps: 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c58c5",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "355b3342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model created with 3 input channels: 55,562,497 parameters\n",
      "   Base channels: 128 (2x larger) | Time embedding: 256 (2x larger)\n",
      "   Model size increase â†’ Better feature representation â†’ Higher SSIM\n"
     ]
    }
   ],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"Build sinusoidal embeddings (from DDPM paper)\"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "    \n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
    "    return emb\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal timestep embedding with learned projection\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        emb = get_timestep_embedding(t, self.dim)\n",
    "        return self.fc(emb)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Residual block with time conditioning\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "        ng_in = max(1, in_ch // 4)\n",
    "        ng_out = max(1, out_ch // 4)\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(ng_in, in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(ng_out, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        \n",
    "        self.time_fc = nn.Linear(time_dim, out_ch)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        h = F.silu(self.norm1(x))\n",
    "        h = self.conv1(h)\n",
    "        h = h + self.time_fc(t_emb)[:, :, None, None]\n",
    "        h = F.silu(self.norm2(h))\n",
    "        h = self.conv2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class FastDDPM(nn.Module):\n",
    "    \"\"\"Fast DDPM UNet for conditional denoising\"\"\"\n",
    "    def __init__(self, in_ch=3, out_ch=1, base_ch=64, time_dim=128):\n",
    "        super().__init__()\n",
    "        self.time_emb = TimeEmbedding(time_dim)\n",
    "        self.init_conv = nn.Conv2d(in_ch, base_ch, 3, padding=1)\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ResBlock(base_ch, base_ch * 2, time_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ResBlock(base_ch * 2, base_ch * 4, time_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ResBlock(base_ch * 4, base_ch * 8, time_dim)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResBlock(base_ch * 8, base_ch * 8, time_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv3 = nn.ConvTranspose2d(base_ch * 8, base_ch * 4, 2, 2)\n",
    "        self.dec3 = ResBlock(base_ch * 4 + base_ch * 8, base_ch * 4, time_dim)\n",
    "    \n",
    "        self.upconv2 = nn.ConvTranspose2d(base_ch * 4, base_ch * 2, 2, 2)\n",
    "        self.dec2 = ResBlock(base_ch * 2 + base_ch * 4, base_ch * 2, time_dim)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(base_ch * 2, base_ch, 2, 2)\n",
    "        self.dec1 = ResBlock(base_ch + base_ch * 2, base_ch, time_dim)\n",
    "        \n",
    "        ng_final = max(1, base_ch // 4)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(ng_final, base_ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch, out_ch, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_emb(t)\n",
    "        h = self.init_conv(x)\n",
    "        \n",
    "        e1 = self.enc1(h, t_emb)\n",
    "        h = self.pool1(e1)\n",
    "        e2 = self.enc2(h, t_emb)\n",
    "        h = self.pool2(e2)\n",
    "        e3 = self.enc3(h, t_emb)\n",
    "        h = self.pool3(e3)\n",
    "        \n",
    "        h = self.bottleneck(h, t_emb)\n",
    "        \n",
    "        h = self.upconv3(h)\n",
    "        h = torch.cat([h, e3], dim=1)\n",
    "        h = self.dec3(h, t_emb)\n",
    "        \n",
    "        h = self.upconv2(h)\n",
    "        h = torch.cat([h, e2], dim=1)\n",
    "        h = self.dec2(h, t_emb)\n",
    "        \n",
    "        h = self.upconv1(h)\n",
    "        h = torch.cat([h, e1], dim=1)\n",
    "        h = self.dec1(h, t_emb)\n",
    "        \n",
    "        return self.final(h)\n",
    "\n",
    "model = FastDDPM(in_ch=3, out_ch=1, base_ch=128, time_dim=256).to(DEVICE)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ… Model created with 3 input channels: {num_params:,} parameters\")\n",
    "print(f\"   Base channels: 128 (2x larger) | Time embedding: 256 (2x larger)\")\n",
    "print(f\"   Model size increase â†’ Better feature representation â†’ Higher SSIM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b9725",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec646fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scheduler tensors moved to device\n",
      "   Using 10 inference steps for high-quality generation\n",
      "âœ… Checkpoint utilities defined\n",
      "\n",
      "ðŸ” Checking for existing checkpoints...\n",
      "ðŸ“‚ Found checkpoint: fastddpm_checkpoint_inc_ch_20.pt\n",
      "âœ… Loaded checkpoint from epoch 20\n",
      "   Resuming training from epoch 21\n",
      "   Best validation loss so far: 0.2275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler_device = DDPMScheduler(num_timesteps=1000, num_inference_steps=10, scheduler_type='non-uniform', beta_schedule='cosine')\n",
    "\n",
    "# Move all scheduler tensors to device\n",
    "scheduler_device.betas = scheduler_device.betas.to(DEVICE)\n",
    "scheduler_device.alphas = scheduler_device.alphas.to(DEVICE)\n",
    "scheduler_device.alphas_cumprod = scheduler_device.alphas_cumprod.to(DEVICE)\n",
    "scheduler_device.sqrt_alphas_cumprod = scheduler_device.sqrt_alphas_cumprod.to(DEVICE)\n",
    "scheduler_device.sqrt_one_minus_alphas_cumprod = scheduler_device.sqrt_one_minus_alphas_cumprod.to(DEVICE)\n",
    "scheduler_device.timesteps = scheduler_device.timesteps.to(DEVICE)\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(\"âœ… Scheduler tensors moved to device\")\n",
    "print(f\"   Using {len(scheduler_device.timesteps)} inference steps for high-quality generation\")\n",
    "\n",
    "# ========== Checkpoint Utilities ==========\n",
    "def get_latest_checkpoint(checkpoint_dir, prefix='fastddpm_checkpoint_inc_ch'):\n",
    "    \"\"\"Get the latest checkpoint file by epoch number\"\"\"\n",
    "    from pathlib import Path\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_files = list(checkpoint_dir.glob(f'{prefix}_*.pt'))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    checkpoints_with_epochs = []\n",
    "    for ckpt in checkpoint_files:\n",
    "        try:\n",
    "            epoch = int(ckpt.stem.split('_')[-1])\n",
    "            checkpoints_with_epochs.append((epoch, ckpt))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    if not checkpoints_with_epochs:\n",
    "        return None\n",
    "    \n",
    "    latest_epoch, latest_ckpt = max(checkpoints_with_epochs, key=lambda x: x[0])\n",
    "    return latest_ckpt, latest_epoch\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"Load checkpoint and return starting epoch and training state\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "    history = checkpoint.get('history', {'epoch': [], 'train_loss': [], 'val_loss': []})\n",
    "    best_loss = checkpoint.get('best_loss', float('inf'))\n",
    "    \n",
    "    return start_epoch, history, best_loss\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, history, best_loss, checkpoint_path):\n",
    "    \"\"\"Save checkpoint\"\"\"\n",
    "    from datetime import datetime\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'history': history,\n",
    "        'best_loss': best_loss,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(\"âœ… Checkpoint utilities defined\")\n",
    "\n",
    "# Check for existing checkpoint\n",
    "print(\"\\nðŸ” Checking for existing checkpoints...\")\n",
    "latest_ckpt_info = get_latest_checkpoint(CHECKPOINT_DIR, prefix='fastddpm_checkpoint_inc_ch')\n",
    "\n",
    "if latest_ckpt_info is not None:\n",
    "    latest_ckpt_path, latest_epoch = latest_ckpt_info\n",
    "    print(f\"ðŸ“‚ Found checkpoint: {latest_ckpt_path.name}\")\n",
    "    \n",
    "    start_epoch, history, best_loss = load_checkpoint(\n",
    "        model, optimizer, latest_ckpt_path, DEVICE\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Loaded checkpoint from epoch {latest_epoch}\")\n",
    "    print(f\"   Resuming training from epoch {start_epoch}\")\n",
    "    print(f\"   Best validation loss so far: {best_loss:.4f}\\n\")\n",
    "else:\n",
    "    print(\"ðŸ“­ No checkpoint found - starting fresh training\\n\")\n",
    "    start_epoch = 1\n",
    "    history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "    best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680f867",
   "metadata": {},
   "source": [
    "## 6. Training & Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7fa2ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training & validation functions ready\n"
     ]
    }
   ],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (pre, post), target in tqdm(train_loader, desc=\"Train\", leave=False):\n",
    "        pre = pre.to(DEVICE)\n",
    "        post = post.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        \n",
    "        # Random timesteps for training\n",
    "        batch_size = pre.shape[0]\n",
    "        t_idx = torch.randint(0, len(scheduler_device.timesteps), (batch_size // 2 + 1,), device=DEVICE)\n",
    "        t_idx = torch.cat([t_idx, len(scheduler_device.timesteps) - t_idx - 1], dim=0)[:batch_size]\n",
    "        t = scheduler_device.timesteps[t_idx]\n",
    "        \n",
    "        noise = torch.randn_like(target).to(DEVICE)\n",
    "        x_noisy = scheduler_device.add_noise(target, t, noise)\n",
    "        \n",
    "        x_input = torch.cat([pre, post, x_noisy], dim=1)\n",
    "        pred_noise = model(x_input, t)\n",
    "        \n",
    "        loss = criterion(pred_noise, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        del pre, post, target, noise, x_noisy, x_input, pred_noise, loss, t, t_idx\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate():\n",
    "    \"\"\"Validation with FIXED timesteps for stable loss (FIX #1)\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (pre, post), target in tqdm(val_loader, desc=\"Val\", leave=False):\n",
    "            pre = pre.to(DEVICE)\n",
    "            post = post.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            \n",
    "            batch_size = pre.shape[0]\n",
    "            # FIX: Use FIXED timesteps for validation (not random)\n",
    "            num_timesteps = len(scheduler_device.timesteps)\n",
    "            t_indices = torch.linspace(0, num_timesteps - 1, batch_size).long().to(DEVICE)\n",
    "            t = scheduler_device.timesteps[t_indices]\n",
    "            \n",
    "            noise = torch.randn_like(target).to(DEVICE)\n",
    "            x_noisy = scheduler_device.add_noise(target, t, noise)\n",
    "            \n",
    "            x_input = torch.cat([pre, post, x_noisy], dim=1)\n",
    "            pred_noise = model(x_input, t)\n",
    "            loss = criterion(pred_noise, noise)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            del pre, post, target, noise, x_noisy, x_input, pred_noise, loss, t, t_indices\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "print(\"âœ… Training & validation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18134b3",
   "metadata": {},
   "source": [
    "## 7. Sampling (Reverse Diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69caa23f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sampling function ready (3 samples generated, returning first one)\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def sample(pre, post, num_samples=3):\n",
    "    \"\"\"Generate samples using reverse diffusion with FIXED formula (FIX #2)\"\"\"\n",
    "    model.eval()\n",
    "    batch_size = pre.shape[0]\n",
    "    \n",
    "    generated = []\n",
    "    for sample_idx in range(num_samples):\n",
    "        x_t = torch.randn(batch_size, 1, 256, 256, device=DEVICE, dtype=torch.float32)\n",
    "        \n",
    "        timesteps = scheduler_device.timesteps.tolist()\n",
    "        num_steps = len(timesteps)\n",
    "        \n",
    "        for step_idx in range(num_steps - 1, -1, -1):\n",
    "            t_value = timesteps[step_idx]\n",
    "            t_batch = torch.full((batch_size,), t_value, dtype=torch.long, device=DEVICE)\n",
    "            \n",
    "            x_input = torch.cat([pre.to(DEVICE), post.to(DEVICE), x_t], dim=1)\n",
    "            pred_noise = model(x_input, t_batch)\n",
    "            \n",
    "            # FIX: Use alphas_cumprod consistently (not alphas)\n",
    "            alpha_t = scheduler_device.alphas_cumprod[t_value]\n",
    "            alpha_t_prev = scheduler_device.alphas_cumprod[timesteps[step_idx - 1]] if step_idx > 0 else torch.tensor(1.0, device=DEVICE)\n",
    "            \n",
    "            # Posterior variance (DDPM formula)\n",
    "            beta_t = 1.0 - scheduler_device.alphas[t_value]\n",
    "            posterior_var = (1 - alpha_t_prev) / (1 - alpha_t) * beta_t\n",
    "            posterior_var = torch.clamp(posterior_var, min=1e-20)\n",
    "            \n",
    "            # Reverse step with corrected formula\n",
    "            x_t = (1.0 / torch.sqrt(alpha_t)) * (\n",
    "                x_t - (1 - alpha_t) / torch.sqrt(1 - alpha_t) * pred_noise\n",
    "            )\n",
    "            \n",
    "            if step_idx > 0:\n",
    "                noise = torch.randn_like(x_t, device=DEVICE)\n",
    "                x_t = x_t + torch.sqrt(posterior_var) * noise\n",
    "        \n",
    "        generated.append(x_t)\n",
    "    \n",
    "    # Return FIRST sample from 3 generated (preserves field details without averaging)\n",
    "    # Change to generated[-1] to use LAST sample instead\n",
    "    return generated[0]  # (B, 1, H, W)\n",
    "\n",
    "print(\"âœ… Sampling function ready (3 samples generated, returning first one)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f9997",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cf287c3-7708-41e6-b567-e4801ea1f6e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Training Loop\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20 | Train: 0.1312 | Val: 0.2441 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/20 | Train: 0.1248 | Val: 0.2390 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/20 | Train: 0.1232 | Val: 0.2360 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/20 | Train: 0.1201 | Val: 0.2349 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/20 | Train: 0.1206 | Val: 0.2333 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/20 | Train: 0.1203 | Val: 0.2319 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/20 | Train: 0.1171 | Val: 0.2313 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/20 | Train: 0.1181 | Val: 0.2321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/20 | Train: 0.1169 | Val: 0.2305 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train: 0.1189 | Val: 0.2298 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train: 0.1166 | Val: 0.2294 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train: 0.1185 | Val: 0.2292 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train: 0.1186 | Val: 0.2311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train: 0.1182 | Val: 0.2286 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train: 0.1138 | Val: 0.2285 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train: 0.1170 | Val: 0.2285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train: 0.1156 | Val: 0.2279 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train: 0.1175 | Val: 0.2277 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train: 0.1176 | Val: 0.2275 âœ… (best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  21%|â–ˆâ–ˆ        | 674/3221 [00:39<02:20, 18.10it/s]      IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Starting Training Loop\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "best_loss = float('inf')\n",
    "start_epoch = 1\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss = train_epoch()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch:2d}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\", end=\"\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/fastddpm_inc_ch_best.pt')\n",
    "        print(\" âœ… (best)\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Save checkpoint after every epoch\n",
    "    checkpoint_path = f'{CHECKPOINT_DIR}/fastddpm_checkpoint_inc_ch_{epoch}.pt'\n",
    "    save_checkpoint(model, optimizer, epoch, history, best_loss, checkpoint_path)\n",
    "\n",
    "with open(f'{results_dir}/fastddpm_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Training complete! Best val loss: {best_loss:.4f}\")\n",
    "print(f\"ðŸ“Š Final history saved to fastddpm_history.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d308d9-5680-4acb-a472-64927bd33935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ Starting Training Loop\n",
      "============================================================\n",
      "\n",
      "Training 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 | Train: 0.1167 | Val: 0.2274 âœ… (best)\n",
      "Training 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 | Train: 0.1174 | Val: 0.2271 âœ… (best)\n",
      "Training 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 | Train: 0.1164 | Val: 0.2268 âœ… (best)\n",
      "Training 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 | Train: 0.1144 | Val: 0.2302\n",
      "Training 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 | Train: 0.1152 | Val: 0.2266 âœ… (best)\n",
      "Training 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 | Train: 0.1159 | Val: 0.2263 âœ… (best)\n",
      "Training 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 | Train: 0.1143 | Val: 0.2263\n",
      "Training 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 | Train: 0.1151 | Val: 0.2270\n",
      "Training 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 | Train: 0.1147 | Val: 0.2263 âœ… (best)\n",
      "Training 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 | Train: 0.1153 | Val: 0.2261 âœ… (best)\n",
      "Training 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 | Train: 0.1152 | Val: 0.2261 âœ… (best)\n",
      "Training 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 | Train: 0.1149 | Val: 0.2258 âœ… (best)\n",
      "Training 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 | Train: 0.1147 | Val: 0.2256 âœ… (best)\n",
      "Training 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 | Train: 0.1151 | Val: 0.2267\n",
      "Training 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 | Train: 0.1162 | Val: 0.2257\n",
      "Training 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 | Train: 0.1149 | Val: 0.2253 âœ… (best)\n",
      "Training 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 | Train: 0.1167 | Val: 0.2252 âœ… (best)\n",
      "Training 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 | Train: 0.1161 | Val: 0.2252 âœ… (best)\n",
      "Training 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 | Train: 0.1150 | Val: 0.2251 âœ… (best)\n",
      "Training 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 | Train: 0.1142 | Val: 0.2257\n",
      "\n",
      "âœ… Training complete! Best val loss: 0.2251\n",
      "ðŸ“Š Final history saved to fastddpm_history.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ Starting Training Loop\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "best_loss = float('inf')\n",
    "#start_epoch = 1\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    print(f'Training {epoch}')\n",
    "    train_loss = train_epoch()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch:2d}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\", end=\"\")\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{CHECKPOINT_DIR}/fastddpm_inc_ch_best.pt')\n",
    "        print(\" âœ… (best)\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Save checkpoint after every epoch\n",
    "    checkpoint_path = f'{CHECKPOINT_DIR}/fastddpm_checkpoint_inc_ch_{epoch}.pt'\n",
    "    save_checkpoint(model, optimizer, epoch, history, best_loss, checkpoint_path)\n",
    "\n",
    "with open(f'{results_dir}/fastddpm_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Training complete! Best val loss: {best_loss:.4f}\")\n",
    "print(f\"ðŸ“Š Final history saved to fastddpm_history.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15f481",
   "metadata": {},
   "source": [
    "## 9. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cb3316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Plot saved\n",
      "Best validation loss: 0.2251 at epoch 39\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history['epoch'], history['train_loss'], 'o-', label='Train Loss', linewidth=2, markersize=4)\n",
    "plt.plot(history['epoch'], history['val_loss'], 's-', label='Val Loss', linewidth=2, markersize=4)\n",
    "\n",
    "best_val_loss = min(history['val_loss'])\n",
    "best_epoch = history['epoch'][history['val_loss'].index(best_val_loss)]\n",
    "plt.axhline(y=best_val_loss, color='g', linestyle='--', alpha=0.5, label=f'Best Val Loss: {best_val_loss:.4f} (Epoch {best_epoch})')\n",
    "plt.axvline(x=best_epoch, color='g', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('FastDDPM Training History (With Fixes)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/fastddpm_training_inc_ch.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Plot saved\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea5989",
   "metadata": {},
   "source": [
    "## 10. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49714e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4560/4560 [34:16<00:00,  2.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FastDDPM Test Set Performance (with fixes)\n",
      "============================================================\n",
      "SSIM: 0.6371 Â± 0.0759\n",
      "PSNR: 22.85 Â± 2.68 dB\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/fastddpm_inc_ch_best.pt', map_location=DEVICE))\n",
    "\n",
    "ssim_scores = []\n",
    "psnr_scores = []\n",
    "predictions = []\n",
    "targets_list = []\n",
    "\n",
    "print(\"Running evaluation on test set...\")\n",
    "for (pre, post), target in tqdm(test_loader, desc=\"Testing\"):\n",
    "    generated = sample(pre, post, num_samples=3)  # (B, 1, H, W)\n",
    "    predictions.append(generated.cpu())\n",
    "    targets_list.append(target.cpu())\n",
    "    \n",
    "    # FIX: Proper tensor shape handling\n",
    "    pred = generated.squeeze(1).cpu().numpy()  # (B, H, W)\n",
    "    gt = target.squeeze(1).cpu().numpy()       # (B, H, W)\n",
    "    \n",
    "    for i in range(len(gt)):\n",
    "        gt_norm = (gt[i] - gt[i].min()) / (gt[i].max() - gt[i].min() + 1e-8)\n",
    "        pred_norm = (pred[i] - pred[i].min()) / (pred[i].max() - pred[i].min() + 1e-8)\n",
    "        \n",
    "        ssim_scores.append(ssim(gt_norm, pred_norm, data_range=1.0))\n",
    "        psnr_scores.append(psnr(gt_norm, pred_norm, data_range=1.0))\n",
    "    \n",
    "    del generated, pred, gt, gt_norm, pred_norm\n",
    "    if DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FastDDPM Test Set Performance (with fixes)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"SSIM: {np.mean(ssim_scores):.4f} Â± {np.std(ssim_scores):.4f}\")\n",
    "print(f\"PSNR: {np.mean(psnr_scores):.2f} Â± {np.std(psnr_scores):.2f} dB\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03cd93be-702f-45d3-8d32-354044a4d105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FastDDPM:\n\tsize mismatch for time_emb.fc.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for time_emb.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for time_emb.fc.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for time_emb.fc.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for init_conv.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 3, 3, 3]).\n\tsize mismatch for init_conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for enc1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for enc1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for enc1.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for enc1.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for enc1.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.time_fc.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for enc1.time_fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.skip.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for enc1.skip.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc2.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for enc2.conv1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for enc2.conv2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.time_fc.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for enc2.time_fc.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.skip.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for enc2.skip.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc3.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 512, 3, 3]).\n\tsize mismatch for enc3.conv1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for enc3.conv2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.time_fc.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for enc3.time_fc.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.skip.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 512, 1, 1]).\n\tsize mismatch for enc3.skip.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for bottleneck.conv1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for bottleneck.conv2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.time_fc.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for bottleneck.time_fc.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for upconv3.weight: copying a param with shape torch.Size([512, 256, 2, 2]) from checkpoint, the shape in current model is torch.Size([1024, 512, 2, 2]).\n\tsize mismatch for upconv3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for dec3.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for dec3.conv1.weight: copying a param with shape torch.Size([256, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1536, 3, 3]).\n\tsize mismatch for dec3.conv1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for dec3.conv2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.time_fc.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for dec3.time_fc.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.skip.weight: copying a param with shape torch.Size([256, 768, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1536, 1, 1]).\n\tsize mismatch for dec3.skip.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for upconv2.weight: copying a param with shape torch.Size([256, 128, 2, 2]) from checkpoint, the shape in current model is torch.Size([512, 256, 2, 2]).\n\tsize mismatch for upconv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for dec2.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for dec2.conv1.weight: copying a param with shape torch.Size([128, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 768, 3, 3]).\n\tsize mismatch for dec2.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for dec2.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.time_fc.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for dec2.time_fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.skip.weight: copying a param with shape torch.Size([128, 384, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 768, 1, 1]).\n\tsize mismatch for dec2.skip.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for upconv1.weight: copying a param with shape torch.Size([128, 64, 2, 2]) from checkpoint, the shape in current model is torch.Size([256, 128, 2, 2]).\n\tsize mismatch for upconv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for dec1.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for dec1.conv1.weight: copying a param with shape torch.Size([64, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 384, 3, 3]).\n\tsize mismatch for dec1.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for dec1.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.time_fc.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for dec1.time_fc.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.skip.weight: copying a param with shape torch.Size([64, 192, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 384, 1, 1]).\n\tsize mismatch for dec1.skip.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for final.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for final.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for final.2.weight: copying a param with shape torch.Size([1, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 128, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load best model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCHECKPOINT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/fastddpm_best.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m predictions = []\n\u001b[32m      5\u001b[39m targets_list = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torch/nn/modules/module.py:2624\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2616\u001b[39m         error_msgs.insert(\n\u001b[32m   2617\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2620\u001b[39m             ),\n\u001b[32m   2621\u001b[39m         )\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2627\u001b[39m         )\n\u001b[32m   2628\u001b[39m     )\n\u001b[32m   2629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for FastDDPM:\n\tsize mismatch for time_emb.fc.0.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for time_emb.fc.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for time_emb.fc.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 512]).\n\tsize mismatch for time_emb.fc.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for init_conv.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 3, 3, 3]).\n\tsize mismatch for init_conv.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for enc1.norm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for enc1.norm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for enc1.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for enc1.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for enc1.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.time_fc.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for enc1.time_fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc1.skip.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for enc1.skip.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc2.norm1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc2.norm1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for enc2.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for enc2.conv1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for enc2.conv2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.time_fc.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for enc2.time_fc.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc2.skip.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for enc2.skip.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc3.norm1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc3.norm1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc3.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 512, 3, 3]).\n\tsize mismatch for enc3.conv1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for enc3.conv2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.time_fc.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for enc3.time_fc.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for enc3.skip.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 512, 1, 1]).\n\tsize mismatch for enc3.skip.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.norm1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.norm1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for bottleneck.conv1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.norm2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.norm2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.conv2.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 3, 3]).\n\tsize mismatch for bottleneck.conv2.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for bottleneck.time_fc.weight: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for bottleneck.time_fc.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for upconv3.weight: copying a param with shape torch.Size([512, 256, 2, 2]) from checkpoint, the shape in current model is torch.Size([1024, 512, 2, 2]).\n\tsize mismatch for upconv3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for dec3.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for dec3.conv1.weight: copying a param with shape torch.Size([256, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1536, 3, 3]).\n\tsize mismatch for dec3.conv1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.norm2.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.norm2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.conv2.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for dec3.conv2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.time_fc.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for dec3.time_fc.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for dec3.skip.weight: copying a param with shape torch.Size([256, 768, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1536, 1, 1]).\n\tsize mismatch for dec3.skip.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for upconv2.weight: copying a param with shape torch.Size([256, 128, 2, 2]) from checkpoint, the shape in current model is torch.Size([512, 256, 2, 2]).\n\tsize mismatch for upconv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.norm1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for dec2.norm1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for dec2.conv1.weight: copying a param with shape torch.Size([128, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 768, 3, 3]).\n\tsize mismatch for dec2.conv1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.norm2.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.norm2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.conv2.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for dec2.conv2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.time_fc.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for dec2.time_fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for dec2.skip.weight: copying a param with shape torch.Size([128, 384, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 768, 1, 1]).\n\tsize mismatch for dec2.skip.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for upconv1.weight: copying a param with shape torch.Size([128, 64, 2, 2]) from checkpoint, the shape in current model is torch.Size([256, 128, 2, 2]).\n\tsize mismatch for upconv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for dec1.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for dec1.conv1.weight: copying a param with shape torch.Size([64, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 384, 3, 3]).\n\tsize mismatch for dec1.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.norm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.norm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for dec1.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.time_fc.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for dec1.time_fc.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for dec1.skip.weight: copying a param with shape torch.Size([64, 192, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 384, 1, 1]).\n\tsize mismatch for dec1.skip.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for final.0.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for final.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for final.2.weight: copying a param with shape torch.Size([1, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 128, 3, 3])."
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(f'{CHECKPOINT_DIR}/fastddpm_best.pt', map_location=DEVICE))\n",
    "\n",
    "predictions = []\n",
    "targets_list = []\n",
    "\n",
    "print(\"Running evaluation on test set...\")\n",
    "for (pre, post), target in tqdm(test_loader, desc=\"Testing\"):\n",
    "    generated = sample(pre, post, num_samples=3)  # (B, 1, H, W)\n",
    "    predictions.append(generated.cpu())\n",
    "    targets_list.append(target.cpu())\n",
    "    \n",
    "    # FIX: Proper tensor shape handling\n",
    "    pred = generated.squeeze(1).cpu().numpy()  # (B, H, W)\n",
    "    gt = target.squeeze(1).cpu().numpy()       # (B, H, W)\n",
    "    \n",
    "    del generated, pred, gt\n",
    "    if DEVICE == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305eef90-0d30-4976-aa6f-10df53c523f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate all predictions and targets\n",
    "all_predictions = torch.cat(predictions, dim=0)  # (N, 1, H, W)\n",
    "all_targets = torch.cat(targets_list, dim=0)      # (N, 1, H, W)\n",
    "\n",
    "# Visualize some predictions\n",
    "n_samples = 4\n",
    "fig, axes = plt.subplots(n_samples, 3, figsize=(12, 4*n_samples))\n",
    "\n",
    "# Calculate global min/max for difference colorbar across all samples\n",
    "all_diffs = []\n",
    "for i in range(n_samples):\n",
    "    pred = all_predictions[i, 0].numpy()\n",
    "    target = all_targets[i, 0].numpy()\n",
    "    diff = np.abs(pred - target)\n",
    "    all_diffs.append(diff)\n",
    "\n",
    "vmin_diff = min([d.min() for d in all_diffs])\n",
    "vmax_diff = max([d.max() for d in all_diffs])\n",
    "\n",
    "for i in range(n_samples):\n",
    "    pred = all_predictions[i, 0].numpy()\n",
    "    target = all_targets[i, 0].numpy()\n",
    "    diff = np.abs(pred - target)\n",
    "    \n",
    "    axes[i, 0].imshow(target, cmap='gray')\n",
    "    axes[i, 0].set_title(f'Target Slice {i+1}', fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(pred, cmap='gray')\n",
    "    axes[i, 1].set_title(f'Predicted Slice {i+1}', fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    im = axes[i, 2].imshow(diff, cmap='hot', vmin=vmin_diff, vmax=vmax_diff)\n",
    "    axes[i, 2].set_title(f'Difference {i+1}', fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "    plt.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "pred_path = results_dir + '/fast_ddpm_predictions_inc_ch.png'\n",
    "plt.savefig(pred_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Predictions visualization saved to {pred_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a55f38-68d8-4e38-930c-d6fc63f54f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmia-2",
   "language": "python",
   "name": "dlmia-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
