{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07ddcce",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34db8b14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Device: cuda\n",
      "   GPU: NVIDIA B200\n",
      "   Memory: 191.51 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Check device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"âœ… Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a9d3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup path to access custom modules\n",
    "root_directory = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(root_directory, 'src'))\n",
    "\n",
    "from ModelDataGenerator import build_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8a5d5",
   "metadata": {},
   "source": [
    "# GPU Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec6b4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Memory Usage:\n",
      "   Allocated: 0.00 GB\n",
      "   Reserved: 0.00 GB\n",
      "   Total Available: 191.51 GB\n",
      "âœ… GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU cache and garbage collect\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"âœ… GPU memory cleared\")\n",
    "\n",
    "# Check initial GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    initial_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved_memory = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"Initial GPU Memory Usage:\")\n",
    "    print(f\"   Allocated: {initial_memory:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved_memory:.2f} GB\")\n",
    "    print(f\"   Total Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef91c0",
   "metadata": {},
   "source": [
    "# DeepCNN Architecture (ResNet-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3654fa98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with skip connection\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57b0ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DeepCNN model defined\n"
     ]
    }
   ],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepCNN Architecture for medical image super-resolution\n",
    "    Based on ResNet with residual blocks for skip connections\n",
    "    \n",
    "    Input: (B, 2, H, W) - prior and posterior slices\n",
    "    Output: (B, 1, H, W) - predicted middle slice\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=2, out_channels=1, num_blocks=[2, 2, 2, 2], base_features=64):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        \n",
    "        self.base_features = base_features\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, base_features, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(base_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(ResidualBlock, base_features, base_features, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(ResidualBlock, base_features, base_features * 2, num_blocks[1], stride=1)\n",
    "        self.layer3 = self._make_layer(ResidualBlock, base_features * 2, base_features * 4, num_blocks[2], stride=1)\n",
    "        self.layer4 = self._make_layer(ResidualBlock, base_features * 4, base_features * 8, num_blocks[3], stride=1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.output_conv = nn.Conv2d(base_features * 8, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_layer(self, block, in_channels, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.output_conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"âœ… DeepCNN model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6eebb3",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee578b50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Total parameters: 11,173,889\n",
      "Trainable parameters: 11,173,889\n",
      "\n",
      "Test forward pass:\n",
      "   Input shape: torch.Size([2, 2, 256, 256])\n",
      "   Output shape: torch.Size([2, 1, 256, 256])\n",
      "   âœ… Model works correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test model initialization\n",
    "model = DeepCNN(in_channels=2, out_channels=1, num_blocks=[2, 2, 2, 2], base_features=64)\n",
    "print(f\"Model created successfully!\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(2, 2, 256, 256)  # Batch size 2, 2 channels, 256x256\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_input)\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Output shape: {test_output.shape}\")\n",
    "print(f\"   âœ… Model works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2982a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba61bc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ DATA CONFIGURATION:\n",
      "   BATCH_SIZE: 4\n",
      "   NUM_WORKERS: 8\n",
      "   AUGMENTATION: True\n",
      "\n",
      "âœ… Data loaders created\n",
      "   Train: 18269 batches\n",
      "   Val: 3221 batches\n",
      "   Test: 4560 batches\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 8\n",
    "AUGMENT = True\n",
    "\n",
    "print(f\"âš¡ DATA CONFIGURATION:\")\n",
    "print(f\"   BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   NUM_WORKERS: {NUM_WORKERS}\")\n",
    "print(f\"   AUGMENTATION: {AUGMENT}\")\n",
    "\n",
    "# Build dataloaders\n",
    "train_loader = build_dataloader(\n",
    "    split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=AUGMENT,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = build_dataloader(\n",
    "    split=\"val\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = build_dataloader(\n",
    "    split=\"test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Data loaders created\")\n",
    "print(f\"   Train: {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_loader)} batches\")\n",
    "print(f\"   Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af96f9",
   "metadata": {},
   "source": [
    "# Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ba7266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "   Epochs: 10\n",
      "   Learning rate: 0.0001\n",
      "   Early stopping patience: 5\n",
      "   Batch size: 4\n",
      "   Model save dir: ../models\n",
      "\n",
      "âœ… Model initialized on cuda\n",
      "   Total parameters: 11,173,889\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "MODEL_SAVE_DIR = Path('../models')\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Model save dir: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# Initialize model, optimizer, and loss\n",
    "model = DeepCNN(in_channels=2, out_channels=1, num_blocks=[2, 2, 2, 2], base_features=64).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"\\nâœ… Model initialized on {DEVICE}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f239d0",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1871a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/10 | Train Loss: 3.2683 | Val Loss: 0.2213(Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/10 | Train Loss: 0.1109 | Val Loss: 0.1461(Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/10 | Train Loss: 0.0916 | Val Loss: 0.1141(Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/10 | Train Loss: 0.0853 | Val Loss: 0.1089(Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/10 | Train Loss: 0.0819 | Val Loss: 0.1002(Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/10 | Train Loss: 0.0796 | Val Loss: 0.1037 (patience: 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7/10 | Train Loss: 0.0779 | Val Loss: 0.1020 (patience: 2/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/10 | Train Loss: 0.0767 | Val Loss: 0.0956(Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9/10 | Train Loss: 0.0758 | Val Loss: 0.1039 (patience: 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/10 | Train Loss: 0.0748 | Val Loss: 0.0957 (patience: 2/5)\n",
      "======================================================================\n",
      "Training completed!\n",
      "Best validation loss: 0.0956\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for (pre, post), target in pbar:\n",
    "        # Stack prior and posterior as input\n",
    "        inputs = torch.cat([pre, post], dim=1).to(device)\n",
    "        targets = target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        del inputs, outputs, targets, loss\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for (pre, post), target in pbar:\n",
    "            inputs = torch.cat([pre, post], dim=1).to(device)\n",
    "            targets = target.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            del inputs, outputs, targets, loss\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "# Training variables\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    val_loss = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\", end=\"\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_SAVE_DIR / 'deepcnn_best.pt')\n",
    "            print(\"(Best)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "            \n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "                break\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab467f3",
   "metadata": {},
   "source": [
    "# Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a87514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded from epoch 8\n",
      "\n",
      "Evaluating on test set...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "        for (pre, post), target in pbar:\n",
    "            inputs = torch.cat([pre, post], dim=1).to(device)\n",
    "            targets = target.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.append(outputs.cpu())\n",
    "            targets_list.append(targets.cpu())\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            del inputs, outputs, targets, loss\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss, predictions, targets_list\n",
    "\n",
    "\n",
    "# Load best model\n",
    "best_model_path = MODEL_SAVE_DIR / 'deepcnn_best.pt'\n",
    "if best_model_path.exists():\n",
    "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Best model loaded from epoch {checkpoint['epoch']}\")\n",
    "else:\n",
    "    print(\"Best model not found, using current model\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\\n\")\n",
    "test_loss, predictions, targets_list = evaluate(model, test_loader, criterion, DEVICE)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563c4f0",
   "metadata": {},
   "source": [
    "# Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045e9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate all predictions and targets\n",
    "all_predictions = torch.cat(predictions, dim=0)  # (N, 1, H, W)\n",
    "all_targets = torch.cat(targets_list, dim=0)      # (N, 1, H, W)\n",
    "\n",
    "# Visualize some predictions\n",
    "n_samples = 4\n",
    "fig, axes = plt.subplots(n_samples, 3, figsize=(12, 4*n_samples))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    pred = all_predictions[i, 0].numpy()\n",
    "    target = all_targets[i, 0].numpy()\n",
    "    diff = np.abs(pred - target)\n",
    "    \n",
    "    axes[i, 0].imshow(target, cmap='gray')\n",
    "    axes[i, 0].set_title(f'Target Slice {i+1}', fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(pred, cmap='gray')\n",
    "    axes[i, 1].set_title(f'Predicted Slice {i+1}', fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    im = axes[i, 2].imshow(diff, cmap='hot')\n",
    "    axes[i, 2].set_title(f'Difference {i+1}', fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "    plt.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "pred_path = MODEL_SAVE_DIR / 'deepcnn_predictions.png'\n",
    "plt.savefig(pred_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Predictions saved to {pred_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8bc1b",
   "metadata": {},
   "source": [
    "# Compute Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056b7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate SSIM and PSNR scores\n",
    "all_predictions_np = all_predictions.numpy()  # (N, 1, H, W)\n",
    "all_targets_np = all_targets.numpy()          # (N, 1, H, W)\n",
    "\n",
    "ssim_scores = []\n",
    "psnr_scores = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING QUALITY METRICS FOR DeepCNN MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(len(all_predictions_np)):\n",
    "    pred = all_predictions_np[i, 0]  # (H, W)\n",
    "    target = all_targets_np[i, 0]     # (H, W)\n",
    "    \n",
    "    # Calculate SSIM\n",
    "    ssim_score = ssim(target, pred, data_range=pred.max() - pred.min())\n",
    "    ssim_scores.append(ssim_score)\n",
    "    \n",
    "    # Calculate PSNR\n",
    "    psnr_score = psnr(target, pred, data_range=pred.max() - pred.min())\n",
    "    psnr_scores.append(psnr_score)\n",
    "\n",
    "ssim_scores = np.array(ssim_scores)\n",
    "psnr_scores = np.array(psnr_scores)\n",
    "\n",
    "print(f\"\\nðŸ“Š DeepCNN MODEL - IMAGE QUALITY METRICS:\")\n",
    "print(f\"\\n{'Metric':<20} {'Mean':<12} {'Std Dev':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(f\"{'-'*68}\")\n",
    "print(f\"{'SSIM':20} {ssim_scores.mean():<12.4f} {ssim_scores.std():<12.4f} {ssim_scores.min():<12.4f} {ssim_scores.max():<12.4f}\")\n",
    "print(f\"{'PSNR (dB)':20} {psnr_scores.mean():<12.4f} {psnr_scores.std():<12.4f} {psnr_scores.min():<12.4f} {psnr_scores.max():<12.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Per-Sample Metrics (first 10 samples):\")\n",
    "for i in range(min(10, len(ssim_scores))):\n",
    "    print(f\"   Sample {i+1:3d}: SSIM = {ssim_scores[i]:.4f}, PSNR = {psnr_scores[i]:.2f} dB\")\n",
    "\n",
    "if len(ssim_scores) > 10:\n",
    "    print(f\"   ... and {len(ssim_scores)-10} more samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa5d1c",
   "metadata": {},
   "source": [
    "# Training History & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92826b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', linewidth=2, marker='o', markersize=3)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2, marker='s', markersize=3)\n",
    "plt.xlabel('Epoch', fontsize=11)\n",
    "plt.ylabel('Loss (MSE)', fontsize=11)\n",
    "plt.title('DeepCNN Training Progress', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2, color='orange')\n",
    "plt.xlabel('Epoch', fontsize=11)\n",
    "plt.ylabel('Loss (MSE)', fontsize=11)\n",
    "plt.title('Validation Loss (Zoomed)', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = MODEL_SAVE_DIR / 'deepcnn_training_curves.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curves saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2ce4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'test_loss': test_loss,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'epochs_trained': len(train_losses),\n",
    "    'metrics': {\n",
    "        'ssim_mean': float(ssim_scores.mean()),\n",
    "        'ssim_std': float(ssim_scores.std()),\n",
    "        'psnr_mean': float(psnr_scores.mean()),\n",
    "        'psnr_std': float(psnr_scores.std())\n",
    "    },\n",
    "    'config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
    "        'augmentation': AUGMENT,\n",
    "        'architecture': 'DeepCNN ResNet-style',\n",
    "        'base_features': 64,\n",
    "        'num_blocks': [2, 2, 2, 2]\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "log_path = MODEL_SAVE_DIR / 'deepcnn_history.json'\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(f\"Training history saved to {log_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"DeepCNN TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"   Model: DeepCNN (ResNet-style, 4 residual layers)\")\n",
    "print(f\"   Epochs trained: {len(train_losses)}\")\n",
    "print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"   Final test loss: {test_loss:.4f}\")\n",
    "print(f\"\\nQuality Metrics:\")\n",
    "print(f\"   SSIM: {ssim_scores.mean():.4f} Â± {ssim_scores.std():.4f}\")\n",
    "print(f\"   PSNR: {psnr_scores.mean():.4f} Â± {psnr_scores.std():.4f} dB\")\n",
    "print(f\"\\nFiles saved to: {MODEL_SAVE_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d3d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmia-2",
   "language": "python",
   "name": "dlmia-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
