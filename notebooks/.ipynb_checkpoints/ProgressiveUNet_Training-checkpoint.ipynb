{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0789ad68",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6c7ab",
   "metadata": {},
   "source": [
    "# Progressive UNet for Medical Image Super-Resolution\n",
    "\n",
    "Progressive multi-stage UNet architecture:\n",
    "- **Stage 1:** Single UNet predicts i+2 (middle slice) from i and i+4\n",
    "- **Stage 2:** Two UNets in parallel:\n",
    "  - UNet2: Predicts i+1 from (i, i+2_generated)\n",
    "  - UNet3: Predicts i+3 from (i+2_generated, i+4)\n",
    "\n",
    "**Loss Strategy:** Multi-scale MSE loss from all three predictions (i+1, i+2, i+3) to compensate for vanishing gradients (Inception-style auxiliary losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43858cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device: cuda\n",
      "   GPU: NVIDIA B200\n",
      "   Memory: 191.51 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Check device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"‚úÖ Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0561547a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup path to access custom modules\n",
    "root_directory = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(root_directory, 'src'))\n",
    "\n",
    "from ModelDataGenerator_ProgressiveUNet import build_progressive_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719fb73",
   "metadata": {},
   "source": [
    "# Progressive UNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d576abf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Progressive UNet Architecture defined\n"
     ]
    }
   ],
   "source": [
    "class UNetBlock(nn.Module):\n",
    "    \"\"\"Basic UNet encoder/decoder block with convolution and skip connection support\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(UNetBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetStage(nn.Module):\n",
    "    \"\"\"\n",
    "    Single UNet stage for Progressive UNet\n",
    "    Input: 2 slices (e.g., i and i+4)\n",
    "    Output: 1 slice prediction (e.g., i+2)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=2, out_channels=1, base_features=64):\n",
    "        super(UNetStage, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = UNetBlock(in_channels, base_features)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc2 = UNetBlock(base_features, base_features * 2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc3 = UNetBlock(base_features * 2, base_features * 4)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc4 = UNetBlock(base_features * 4, base_features * 8)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNetBlock(base_features * 8, base_features * 16)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(base_features * 16, base_features * 8, kernel_size=2, stride=2)\n",
    "        self.dec4 = UNetBlock(base_features * 16, base_features * 8)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(base_features * 8, base_features * 4, kernel_size=2, stride=2)\n",
    "        self.dec3 = UNetBlock(base_features * 8, base_features * 4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(base_features * 4, base_features * 2, kernel_size=2, stride=2)\n",
    "        self.dec2 = UNetBlock(base_features * 4, base_features * 2)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(base_features * 2, base_features, kernel_size=2, stride=2)\n",
    "        self.dec1 = UNetBlock(base_features * 2, base_features)\n",
    "        \n",
    "        # Final output\n",
    "        self.final = nn.Conv2d(base_features, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        x = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.enc2(x)\n",
    "        x = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.enc3(x)\n",
    "        x = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.enc4(x)\n",
    "        x = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.upconv4(x)\n",
    "        x = torch.cat([x, enc4], dim=1)\n",
    "        x = self.dec4(x)\n",
    "        \n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat([x, enc3], dim=1)\n",
    "        x = self.dec3(x)\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, enc2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        \n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat([x, enc1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ProgressiveUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Progressive UNet with 3 stages:\n",
    "    - Stage 1: UNet1(i, i+4) -> i+2_pred\n",
    "    - Stage 2A: UNet2(i, i+2_pred) -> i+1_pred\n",
    "    - Stage 2B: UNet3(i+2_pred, i+4) -> i+3_pred\n",
    "    \"\"\"\n",
    "    def __init__(self, base_features=64):\n",
    "        super(ProgressiveUNet, self).__init__()\n",
    "        \n",
    "        # Stage 1: Predict middle slice (i+2)\n",
    "        self.unet1 = UNetStage(in_channels=2, out_channels=1, base_features=base_features)\n",
    "        \n",
    "        # Stage 2: Predict adjacent slices\n",
    "        self.unet2 = UNetStage(in_channels=2, out_channels=1, base_features=base_features)  # (i, i+2) -> i+1\n",
    "        self.unet3 = UNetStage(in_channels=2, out_channels=1, base_features=base_features)  # (i+2, i+4) -> i+3\n",
    "    \n",
    "    def forward(self, slices):\n",
    "        \"\"\"\n",
    "        Input: slices of shape (B, 5, H, W)\n",
    "            - slices[:, 0] = i\n",
    "            - slices[:, 1] = i+1 (ground truth)\n",
    "            - slices[:, 2] = i+2 (ground truth)\n",
    "            - slices[:, 3] = i+3 (ground truth)\n",
    "            - slices[:, 4] = i+4\n",
    "        \n",
    "        Returns:\n",
    "            - pred_i2: Predicted i+2\n",
    "            - pred_i1: Predicted i+1\n",
    "            - pred_i3: Predicted i+3\n",
    "        \"\"\"\n",
    "        batch_size = slices.shape[0]\n",
    "        \n",
    "        # Extract individual slices\n",
    "        i = slices[:, 0:1, :, :]      # (B, 1, H, W)\n",
    "        i_plus_1 = slices[:, 1:2, :, :]  # (B, 1, H, W)\n",
    "        i_plus_2 = slices[:, 2:3, :, :]  # (B, 1, H, W)\n",
    "        i_plus_3 = slices[:, 3:4, :, :]  # (B, 1, H, W)\n",
    "        i_plus_4 = slices[:, 4:5, :, :]  # (B, 1, H, W)\n",
    "        \n",
    "        # Stage 1: Predict i+2 from (i, i+4)\n",
    "        input_stage1 = torch.cat([i, i_plus_4], dim=1)  # (B, 2, H, W)\n",
    "        pred_i_plus_2 = self.unet1(input_stage1)  # (B, 1, H, W)\n",
    "        \n",
    "        # Stage 2A: Predict i+1 from (i, predicted i+2)\n",
    "        input_stage2a = torch.cat([i, pred_i_plus_2], dim=1)  # (B, 2, H, W)\n",
    "        pred_i_plus_1 = self.unet2(input_stage2a)  # (B, 1, H, W)\n",
    "        \n",
    "        # Stage 2B: Predict i+3 from (predicted i+2, i+4)\n",
    "        input_stage2b = torch.cat([pred_i_plus_2, i_plus_4], dim=1)  # (B, 2, H, W)\n",
    "        pred_i_plus_3 = self.unet3(input_stage2b)  # (B, 1, H, W)\n",
    "        \n",
    "        return pred_i_plus_1, pred_i_plus_2, pred_i_plus_3\n",
    "\n",
    "print(\"‚úÖ Progressive UNet Architecture defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27463ac3",
   "metadata": {},
   "source": [
    "# Multi-Scale Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bd21a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-Scale MSE Loss defined\n"
     ]
    }
   ],
   "source": [
    "class MultiScaleMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale MSE Loss for Progressive UNet\n",
    "    Combines losses from all three predictions with different weights\n",
    "    to compensate for vanishing gradient problem (Inception-style auxiliary losses)\n",
    "    \n",
    "    Loss = w1 * MSE(pred_i+1, gt_i+1) + \n",
    "           w2 * MSE(pred_i+2, gt_i+2) + \n",
    "           w3 * MSE(pred_i+3, gt_i+3)\n",
    "    \"\"\"\n",
    "    def __init__(self, weights=None):\n",
    "        super(MultiScaleMSELoss, self).__init__()\n",
    "        \n",
    "        # Default weights for auxiliary losses (higher weight for earlier/middle predictions)\n",
    "        if weights is None:\n",
    "            self.weights = {\n",
    "                'w_i1': 0.5,   # i+1 prediction weight\n",
    "                'w_i2': 1.0,   # i+2 prediction weight (main target)\n",
    "                'w_i3': 0.5    # i+3 prediction weight\n",
    "            }\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred_i1, pred_i2, pred_i3, gt_i1, gt_i2, gt_i3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_i1, pred_i2, pred_i3: Predictions for i+1, i+2, i+3 (B, 1, H, W)\n",
    "            gt_i1, gt_i2, gt_i3: Ground truth for i+1, i+2, i+3 (B, 1, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Total weighted loss\n",
    "        \"\"\"\n",
    "        loss_i1 = self.mse_loss(pred_i1, gt_i1)\n",
    "        loss_i2 = self.mse_loss(pred_i2, gt_i2)\n",
    "        loss_i3 = self.mse_loss(pred_i3, gt_i3)\n",
    "        \n",
    "        total_loss = (self.weights['w_i1'] * loss_i1 + \n",
    "                      self.weights['w_i2'] * loss_i2 + \n",
    "                      self.weights['w_i3'] * loss_i3)\n",
    "        \n",
    "        return total_loss, {\n",
    "            'loss_i1': loss_i1.item(),\n",
    "            'loss_i2': loss_i2.item(),\n",
    "            'loss_i3': loss_i3.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Multi-Scale MSE Loss defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b62506",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf6db76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° DATA CONFIGURATION:\n",
      "   BATCH_SIZE: 4\n",
      "   NUM_WORKERS: 4\n",
      "   AUGMENTATION: True\n",
      "\n",
      "‚úÖ Data loaders created\n",
      "   Train: 8974 batches\n",
      "   Val: 1582 batches\n",
      "   Test: 2240 batches\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "AUGMENT = True\n",
    "\n",
    "print(f\"‚ö° DATA CONFIGURATION:\")\n",
    "print(f\"   BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   NUM_WORKERS: {NUM_WORKERS}\")\n",
    "print(f\"   AUGMENTATION: {AUGMENT}\")\n",
    "\n",
    "# Build dataloaders\n",
    "train_loader = build_progressive_dataloader(\n",
    "    split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=AUGMENT,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = build_progressive_dataloader(\n",
    "    split=\"val\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = build_progressive_dataloader(\n",
    "    split=\"test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaders created\")\n",
    "print(f\"   Train: {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_loader)} batches\")\n",
    "print(f\"   Test: {len(test_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c148326f",
   "metadata": {},
   "source": [
    "# Training Configuration & Checkpoint Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3770e761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "   Epochs: 30\n",
      "   Learning Rate: 0.0005\n",
      "   Early stopping patience: 5\n",
      "   Batch size: 4\n",
      "\n",
      "Loss Weights:\n",
      "   i+1 prediction: 0.5\n",
      "   i+2 prediction: 1.0\n",
      "   i+3 prediction: 0.5\n",
      "\n",
      "‚úÖ Model initialized on cuda\n",
      "   Total parameters: 93,111,171\n",
      "   - UNet1 (Stage 1): 31,037,057\n",
      "   - UNet2 (Stage 2A): 31,037,057\n",
      "   - UNet3 (Stage 2B): 31,037,057\n",
      "‚úÖ Checkpoint utilities loaded\n",
      "\n",
      "üîç Checking for existing checkpoints...\n",
      "üìÇ Found checkpoint: progressive_unet_checkpoint_17.pt\n",
      "‚úÖ Loaded checkpoint from epoch 17\n",
      "   Resuming training from epoch 18\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 5e-4\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "MODEL_SAVE_DIR = Path('../models')\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_SAVE_DIR = Path('../results')\n",
    "RESULTS_SAVE_DIR.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Loss weights for multi-scale losses\n",
    "LOSS_WEIGHTS = {\n",
    "    'w_i1': 0.5,   # Weight for i+1 prediction\n",
    "    'w_i2': 1.0,   # Weight for i+2 prediction (main)\n",
    "    'w_i3': 0.5    # Weight for i+3 prediction\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nLoss Weights:\")\n",
    "print(f\"   i+1 prediction: {LOSS_WEIGHTS['w_i1']}\")\n",
    "print(f\"   i+2 prediction: {LOSS_WEIGHTS['w_i2']}\")\n",
    "print(f\"   i+3 prediction: {LOSS_WEIGHTS['w_i3']}\")\n",
    "\n",
    "# Initialize model\n",
    "model = ProgressiveUNet(base_features=64).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss function\n",
    "criterion = MultiScaleMSELoss(weights=LOSS_WEIGHTS)\n",
    "\n",
    "print(f\"\\n‚úÖ Model initialized on {DEVICE}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   - UNet1 (Stage 1): {sum(p.numel() for p in model.unet1.parameters()):,}\")\n",
    "print(f\"   - UNet2 (Stage 2A): {sum(p.numel() for p in model.unet2.parameters()):,}\")\n",
    "print(f\"   - UNet3 (Stage 2B): {sum(p.numel() for p in model.unet3.parameters()):,}\")\n",
    "\n",
    "# ========== Checkpoint Utilities ==========\n",
    "def get_latest_checkpoint(checkpoint_dir, prefix='progressive_unet_checkpoint'):\n",
    "    \"\"\"Get the latest checkpoint file by epoch number\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_files = list(checkpoint_dir.glob(f'{prefix}_*.pt'))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    checkpoints_with_epochs = []\n",
    "    for ckpt in checkpoint_files:\n",
    "        try:\n",
    "            epoch = int(ckpt.stem.split('_')[-1])\n",
    "            checkpoints_with_epochs.append((epoch, ckpt))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    if not checkpoints_with_epochs:\n",
    "        return None\n",
    "    \n",
    "    latest_epoch, latest_ckpt = max(checkpoints_with_epochs, key=lambda x: x[0])\n",
    "    return latest_ckpt, latest_epoch\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"Load checkpoint and return starting epoch\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "    return start_epoch, checkpoint\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_losses, val_losses, checkpoint_path):\n",
    "    \"\"\"Save checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(\"‚úÖ Checkpoint utilities loaded\")\n",
    "\n",
    "# ========== Check for Existing Checkpoint ==========\n",
    "print(\"\\nüîç Checking for existing checkpoints...\")\n",
    "latest_ckpt_info = get_latest_checkpoint(MODEL_SAVE_DIR, prefix='progressive_unet_checkpoint')\n",
    "\n",
    "if latest_ckpt_info is not None:\n",
    "    latest_ckpt_path, latest_epoch = latest_ckpt_info\n",
    "    print(f\"üìÇ Found checkpoint: {latest_ckpt_path.name}\")\n",
    "    \n",
    "    start_epoch, loaded_ckpt = load_checkpoint(model, optimizer, latest_ckpt_path, DEVICE)\n",
    "    \n",
    "    train_losses = loaded_ckpt.get('train_losses', {'total': [], 'i1': [], 'i2': [], 'i3': []})\n",
    "    val_losses_saved = loaded_ckpt.get('val_losses', {})\n",
    "    \n",
    "    # Ensure all keys exist\n",
    "    train_losses.setdefault('total', [])\n",
    "    train_losses.setdefault('i1', [])\n",
    "    train_losses.setdefault('i2', [])\n",
    "    train_losses.setdefault('i3', [])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded checkpoint from epoch {latest_epoch}\")\n",
    "    print(f\"   Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"üì≠ No checkpoint found - starting fresh training\")\n",
    "    start_epoch = 1\n",
    "    train_losses = {'total': [], 'i1': [], 'i2': [], 'i3': []}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a762e4d",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3ceaa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_i1_sum = 0.0\n",
    "    loss_i2_sum = 0.0\n",
    "    loss_i3_sum = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for slices in pbar:\n",
    "        slices = slices.to(device)  # (B, 5, H, W)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_i1, pred_i2, pred_i3 = model(slices)\n",
    "        \n",
    "        # Extract ground truth\n",
    "        gt_i1 = slices[:, 1:2, :, :]  # (B, 1, H, W)\n",
    "        gt_i2 = slices[:, 2:3, :, :]  # (B, 1, H, W)\n",
    "        gt_i3 = slices[:, 3:4, :, :]  # (B, 1, H, W)\n",
    "        \n",
    "        # Compute multi-scale loss\n",
    "        loss, loss_dict = criterion(pred_i1, pred_i2, pred_i3, gt_i1, gt_i2, gt_i3)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        total_loss += loss.item()\n",
    "        loss_i1_sum += loss_dict['loss_i1']\n",
    "        loss_i2_sum += loss_dict['loss_i2']\n",
    "        loss_i3_sum += loss_dict['loss_i3']\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        del slices, pred_i1, pred_i2, pred_i3, gt_i1, gt_i2, gt_i3, loss\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    num_batches = len(train_loader)\n",
    "    return {\n",
    "        'total': total_loss / num_batches,\n",
    "        'i1': loss_i1_sum / num_batches,\n",
    "        'i2': loss_i2_sum / num_batches,\n",
    "        'i3': loss_i3_sum / num_batches\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    loss_i1_sum = 0.0\n",
    "    loss_i2_sum = 0.0\n",
    "    loss_i3_sum = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for slices in pbar:\n",
    "            slices = slices.to(device)  # (B, 5, H, W)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_i1, pred_i2, pred_i3 = model(slices)\n",
    "            \n",
    "            # Extract ground truth\n",
    "            gt_i1 = slices[:, 1:2, :, :]\n",
    "            gt_i2 = slices[:, 2:3, :, :]\n",
    "            gt_i3 = slices[:, 3:4, :, :]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, loss_dict = criterion(pred_i1, pred_i2, pred_i3, gt_i1, gt_i2, gt_i3)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loss_i1_sum += loss_dict['loss_i1']\n",
    "            loss_i2_sum += loss_dict['loss_i2']\n",
    "            loss_i3_sum += loss_dict['loss_i3']\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            del slices, pred_i1, pred_i2, pred_i3, gt_i1, gt_i2, gt_i3, loss\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    num_batches = len(val_loader)\n",
    "    return {\n",
    "        'total': total_loss / num_batches,\n",
    "        'i1': loss_i1_sum / num_batches,\n",
    "        'i2': loss_i2_sum / num_batches,\n",
    "        'i3': loss_i3_sum / num_batches\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d52d260-0570-43fd-abb8-3b3a459cfec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Epoch 2-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b1f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best validation loss from checkpoint: 0.2346\n",
      "\n",
      "Starting training...\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/10 | Train Loss: 0.2238 (i1: 0.0835, i2: 0.1407, i3: 0.0826) | Val Loss: 0.2180 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/10 | Train Loss: 0.2142 (i1: 0.0800, i2: 0.1347, i3: 0.0790) | Val Loss: 0.2115 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/10 | Train Loss: 0.2086 (i1: 0.0780, i2: 0.1311, i3: 0.0769) | Val Loss: 0.2063 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|‚ñà‚ñà‚ñà‚ñä      | 3469/8974 [37:48<1:10:01,  1.31it/s, loss=0.1990]"
     ]
    }
   ],
   "source": [
    "# Initialize training variables\n",
    "if 'train_losses' not in locals():\n",
    "    train_losses = {'total': [], 'i1': [], 'i2': [], 'i3': []}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Calculate best_val_loss from history if resuming\n",
    "if len(train_losses['total']) > 0:\n",
    "    if 'val_losses_saved' in locals() and val_losses_saved:\n",
    "        best_val_loss = val_losses_saved.get('total', float('inf'))\n",
    "        print(f\"üìä Best validation loss from checkpoint: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss_dict = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss_dict = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses['total'].append(train_loss_dict['total'])\n",
    "    train_losses['i1'].append(train_loss_dict['i1'])\n",
    "    train_losses['i2'].append(train_loss_dict['i2'])\n",
    "    train_losses['i3'].append(train_loss_dict['i3'])\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss_dict['total']:.4f} \"\n",
    "          f\"(i1: {train_loss_dict['i1']:.4f}, i2: {train_loss_dict['i2']:.4f}, i3: {train_loss_dict['i3']:.4f}) | \"\n",
    "          f\"Val Loss: {val_loss_dict['total']:.4f}\", end=\"\")\n",
    "    \n",
    "    # Save checkpoint at every epoch\n",
    "    ckpt_path = MODEL_SAVE_DIR / f'progressive_unet_checkpoint_{epoch}.pt'\n",
    "    save_checkpoint(model, optimizer, epoch, train_losses, val_loss_dict, ckpt_path)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss_dict['total'] < best_val_loss:\n",
    "        best_val_loss = val_loss_dict['total']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss_dict['total'],\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_loss_dict\n",
    "        }\n",
    "        torch.save(checkpoint, MODEL_SAVE_DIR / 'progressive_unet_best.pt')\n",
    "        print(\" ‚úÖ (Best)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "        \n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "            break\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19feae-a145-469b-bf2a-cb9d0cb4cec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Epoch 4-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc2b03-0974-4490-b098-732ed022dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best validation loss from checkpoint: 0.2063\n",
      "\n",
      "Starting training...\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/20 | Train Loss: 0.2045 (i1: 0.0766, i2: 0.1285, i3: 0.0754) | Val Loss: 0.2047 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/20 | Train Loss: 0.2016 (i1: 0.0756, i2: 0.1266, i3: 0.0744) | Val Loss: 0.2011 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7/20 | Train Loss: 0.1991 (i1: 0.0747, i2: 0.1250, i3: 0.0735) | Val Loss: 0.2003 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/20 | Train Loss: 0.1971 (i1: 0.0740, i2: 0.1238, i3: 0.0727) | Val Loss: 0.1978 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9/20 | Train Loss: 0.1955 (i1: 0.0734, i2: 0.1227, i3: 0.0721) | Val Loss: 0.1989 (patience: 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/20 | Train Loss: 0.1941 (i1: 0.0729, i2: 0.1218, i3: 0.0716) | Val Loss: 0.1957 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11/20 | Train Loss: 0.1928 (i1: 0.0724, i2: 0.1210, i3: 0.0711) | Val Loss: 0.1946 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12/20 | Train Loss: 0.1915 (i1: 0.0720, i2: 0.1202, i3: 0.0707) | Val Loss: 0.1934 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13/20 | Train Loss: 0.1905 (i1: 0.0716, i2: 0.1196, i3: 0.0703) | Val Loss: 0.1931 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14/20 | Train Loss: 0.1895 (i1: 0.0713, i2: 0.1189, i3: 0.0700) | Val Loss: 0.1921 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/20 | Train Loss: 0.1886 (i1: 0.0709, i2: 0.1184, i3: 0.0697) | Val Loss: 0.1916 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16/20 | Train Loss: 0.1878 (i1: 0.0706, i2: 0.1178, i3: 0.0693) | Val Loss: 0.1914 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17/20 | Train Loss: 0.1870 (i1: 0.0703, i2: 0.1173, i3: 0.0690) | Val Loss: 0.1908 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 3301/8974 [16:31<22:30,  4.20it/s, loss=0.2034]  "
     ]
    }
   ],
   "source": [
    "# Initialize training variables\n",
    "if 'train_losses' not in locals():\n",
    "    train_losses = {'total': [], 'i1': [], 'i2': [], 'i3': []}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Calculate best_val_loss from history if resuming\n",
    "if len(train_losses['total']) > 0:\n",
    "    if 'val_losses_saved' in locals() and val_losses_saved:\n",
    "        best_val_loss = val_losses_saved.get('total', float('inf'))\n",
    "        print(f\"üìä Best validation loss from checkpoint: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss_dict = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss_dict = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses['total'].append(train_loss_dict['total'])\n",
    "    train_losses['i1'].append(train_loss_dict['i1'])\n",
    "    train_losses['i2'].append(train_loss_dict['i2'])\n",
    "    train_losses['i3'].append(train_loss_dict['i3'])\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss_dict['total']:.4f} \"\n",
    "          f\"(i1: {train_loss_dict['i1']:.4f}, i2: {train_loss_dict['i2']:.4f}, i3: {train_loss_dict['i3']:.4f}) | \"\n",
    "          f\"Val Loss: {val_loss_dict['total']:.4f}\", end=\"\")\n",
    "    \n",
    "    # Save checkpoint at every epoch\n",
    "    ckpt_path = MODEL_SAVE_DIR / f'progressive_unet_checkpoint_{epoch}.pt'\n",
    "    save_checkpoint(model, optimizer, epoch, train_losses, val_loss_dict, ckpt_path)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss_dict['total'] < best_val_loss:\n",
    "        best_val_loss = val_loss_dict['total']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss_dict['total'],\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_loss_dict\n",
    "        }\n",
    "        torch.save(checkpoint, MODEL_SAVE_DIR / 'progressive_unet_best.pt')\n",
    "        print(\" ‚úÖ (Best)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "        \n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "            break\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e30b0b-30d4-4992-93b2-7e3335aa760a",
   "metadata": {},
   "source": [
    "## Epoch 17 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689d782-c632-4c4f-8e7e-900b189f0811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best validation loss from checkpoint: 0.1908\n",
      "\n",
      "Starting training...\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18/30 | Train Loss: 0.1862 (i1: 0.0700, i2: 0.1168, i3: 0.0688) | Val Loss: 0.1900 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19/30 | Train Loss: 0.1855 (i1: 0.0698, i2: 0.1163, i3: 0.0685) | Val Loss: 0.1904 (patience: 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/30 | Train Loss: 0.1849 (i1: 0.0695, i2: 0.1160, i3: 0.0683) | Val Loss: 0.1905 (patience: 2/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  21/30 | Train Loss: 0.1842 (i1: 0.0693, i2: 0.1156, i3: 0.0680) | Val Loss: 0.1898 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22/30 | Train Loss: 0.1836 (i1: 0.0691, i2: 0.1152, i3: 0.0678) | Val Loss: 0.1889 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23/30 | Train Loss: 0.1831 (i1: 0.0689, i2: 0.1148, i3: 0.0676) | Val Loss: 0.1886 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 5791/8974 [37:17<25:14,  2.10it/s, loss=0.1990]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  24/30 | Train Loss: 0.1825 (i1: 0.0687, i2: 0.1145, i3: 0.0674) | Val Loss: 0.1884 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/30 | Train Loss: 0.1820 (i1: 0.0685, i2: 0.1141, i3: 0.0672) | Val Loss: 0.1883 ‚úÖ (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4800/8974 [31:10<24:01,  2.89it/s, loss=0.1954]  "
     ]
    }
   ],
   "source": [
    "# Initialize training variables\n",
    "if 'train_losses' not in locals():\n",
    "    train_losses = {'total': [], 'i1': [], 'i2': [], 'i3': []}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Calculate best_val_loss from history if resuming\n",
    "if len(train_losses['total']) > 0:\n",
    "    if 'val_losses_saved' in locals() and val_losses_saved:\n",
    "        best_val_loss = val_losses_saved.get('total', float('inf'))\n",
    "        print(f\"üìä Best validation loss from checkpoint: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss_dict = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss_dict = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses['total'].append(train_loss_dict['total'])\n",
    "    train_losses['i1'].append(train_loss_dict['i1'])\n",
    "    train_losses['i2'].append(train_loss_dict['i2'])\n",
    "    train_losses['i3'].append(train_loss_dict['i3'])\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss_dict['total']:.4f} \"\n",
    "          f\"(i1: {train_loss_dict['i1']:.4f}, i2: {train_loss_dict['i2']:.4f}, i3: {train_loss_dict['i3']:.4f}) | \"\n",
    "          f\"Val Loss: {val_loss_dict['total']:.4f}\", end=\"\")\n",
    "    \n",
    "    # Save checkpoint at every epoch\n",
    "    ckpt_path = MODEL_SAVE_DIR / f'progressive_unet_checkpoint_{epoch}.pt'\n",
    "    save_checkpoint(model, optimizer, epoch, train_losses, val_loss_dict, ckpt_path)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss_dict['total'] < best_val_loss:\n",
    "        best_val_loss = val_loss_dict['total']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save best model\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss_dict['total'],\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_loss_dict\n",
    "        }\n",
    "        torch.save(checkpoint, MODEL_SAVE_DIR / 'progressive_unet_best.pt')\n",
    "        print(\" ‚úÖ (Best)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "        \n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "            break\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde318a",
   "metadata": {},
   "source": [
    "# Test Evaluation with Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da44c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set and return predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_pred_i1 = []\n",
    "    all_pred_i2 = []\n",
    "    all_pred_i3 = []\n",
    "    all_gt_i1 = []\n",
    "    all_gt_i2 = []\n",
    "    all_gt_i3 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "        for slices in pbar:\n",
    "            slices = slices.to(device)  # (B, 5, H, W)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_i1, pred_i2, pred_i3 = model(slices)\n",
    "            \n",
    "            # Extract ground truth\n",
    "            gt_i1 = slices[:, 1:2, :, :]\n",
    "            gt_i2 = slices[:, 2:3, :, :]\n",
    "            gt_i3 = slices[:, 3:4, :, :]\n",
    "            \n",
    "            # Collect predictions and targets\n",
    "            all_pred_i1.append(pred_i1.cpu())\n",
    "            all_pred_i2.append(pred_i2.cpu())\n",
    "            all_pred_i3.append(pred_i3.cpu())\n",
    "            all_gt_i1.append(gt_i1.cpu())\n",
    "            all_gt_i2.append(gt_i2.cpu())\n",
    "            all_gt_i3.append(gt_i3.cpu())\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            del slices, pred_i1, pred_i2, pred_i3, gt_i1, gt_i2, gt_i3\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    predictions = {\n",
    "        'i1': torch.cat(all_pred_i1, dim=0),  # (N, 1, H, W)\n",
    "        'i2': torch.cat(all_pred_i2, dim=0),\n",
    "        'i3': torch.cat(all_pred_i3, dim=0)\n",
    "    }\n",
    "    \n",
    "    targets = {\n",
    "        'i1': torch.cat(all_gt_i1, dim=0),  # (N, 1, H, W)\n",
    "        'i2': torch.cat(all_gt_i2, dim=0),\n",
    "        'i3': torch.cat(all_gt_i3, dim=0)\n",
    "    }\n",
    "    \n",
    "    return predictions, targets\n",
    "\n",
    "\n",
    "# Load best model\n",
    "print(\"\\nüîç Loading best model checkpoint for testing...\\n\")\n",
    "best_model_path = MODEL_SAVE_DIR / 'progressive_unet_best.pt'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"‚úÖ Best model loaded from epoch {checkpoint['epoch']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Best model not found, using current model state\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\\n\")\n",
    "predictions, targets = evaluate(model, test_loader, DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Test evaluation completed\")\n",
    "print(f\"   i+1 predictions: {predictions['i1'].shape}\")\n",
    "print(f\"   i+2 predictions: {predictions['i2'].shape}\")\n",
    "print(f\"   i+3 predictions: {predictions['i3'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbbb90",
   "metadata": {},
   "source": [
    "# Quality Metrics: SSIM and PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SSIM and PSNR for each prediction type\n",
    "def compute_metrics(predictions_np, targets_np):\n",
    "    \"\"\"Compute SSIM and PSNR metrics\"\"\"\n",
    "    ssim_scores = []\n",
    "    psnr_scores = []\n",
    "    \n",
    "    for i in range(len(predictions_np)):\n",
    "        pred = predictions_np[i, 0]  # (H, W)\n",
    "        target = targets_np[i, 0]     # (H, W)\n",
    "        \n",
    "        # Calculate SSIM\n",
    "        ssim_score = ssim(target, pred, data_range=pred.max() - pred.min())\n",
    "        ssim_scores.append(ssim_score)\n",
    "        \n",
    "        # Calculate PSNR\n",
    "        psnr_score = psnr(target, pred, data_range=pred.max() - pred.min())\n",
    "        psnr_scores.append(psnr_score)\n",
    "    \n",
    "    return np.array(ssim_scores), np.array(psnr_scores)\n",
    "\n",
    "\n",
    "# Convert to numpy for metrics computation\n",
    "pred_i1_np = predictions['i1'].numpy()\n",
    "pred_i2_np = predictions['i2'].numpy()\n",
    "pred_i3_np = predictions['i3'].numpy()\n",
    "\n",
    "target_i1_np = targets['i1'].numpy()\n",
    "target_i2_np = targets['i2'].numpy()\n",
    "target_i3_np = targets['i3'].numpy()\n",
    "\n",
    "# Compute metrics for each stage\n",
    "ssim_i1, psnr_i1 = compute_metrics(pred_i1_np, target_i1_np)\n",
    "ssim_i2, psnr_i2 = compute_metrics(pred_i2_np, target_i2_np)\n",
    "ssim_i3, psnr_i3 = compute_metrics(pred_i3_np, target_i3_np)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"PROGRESSIVE UNet - QUALITY METRICS ON TEST SET\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(f\"\\n{'Slice':<15} {'Metric':<12} {'Mean':<12} {'Std Dev':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(f\"{'-'*85}\")\n",
    "\n",
    "# i+1 metrics\n",
    "print(f\"{'i+1':<15} {'SSIM':<12} {ssim_i1.mean():<12.4f} {ssim_i1.std():<12.4f} {ssim_i1.min():<12.4f} {ssim_i1.max():<12.4f}\")\n",
    "print(f\"{'':15} {'PSNR (dB)':<12} {psnr_i1.mean():<12.4f} {psnr_i1.std():<12.4f} {psnr_i1.min():<12.4f} {psnr_i1.max():<12.4f}\")\n",
    "\n",
    "# i+2 metrics\n",
    "print(f\"{'i+2':<15} {'SSIM':<12} {ssim_i2.mean():<12.4f} {ssim_i2.std():<12.4f} {ssim_i2.min():<12.4f} {ssim_i2.max():<12.4f}\")\n",
    "print(f\"{'':15} {'PSNR (dB)':<12} {psnr_i2.mean():<12.4f} {psnr_i2.std():<12.4f} {psnr_i2.min():<12.4f} {psnr_i2.max():<12.4f}\")\n",
    "\n",
    "# i+3 metrics\n",
    "print(f\"{'i+3':<15} {'SSIM':<12} {ssim_i3.mean():<12.4f} {ssim_i3.std():<12.4f} {ssim_i3.min():<12.4f} {ssim_i3.max():<12.4f}\")\n",
    "print(f\"{'':15} {'PSNR (dB)':<12} {psnr_i3.mean():<12.4f} {psnr_i3.std():<12.4f} {psnr_i3.min():<12.4f} {psnr_i3.max():<12.4f}\")\n",
    "\n",
    "# Average across all stages\n",
    "avg_ssim = np.mean([ssim_i1.mean(), ssim_i2.mean(), ssim_i3.mean()])\n",
    "avg_psnr = np.mean([psnr_i1.mean(), psnr_i2.mean(), psnr_i3.mean()])\n",
    "\n",
    "print(f\"{'-'*85}\")\n",
    "print(f\"{'Average':<15} {'SSIM':<12} {avg_ssim:<12.4f}\")\n",
    "print(f\"{'':15} {'PSNR (dB)':<12} {avg_psnr:<12.4f}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Per-sample metrics for first 10 samples\n",
    "print(f\"\\nüìà Per-Sample Metrics (first 10 samples):\")\n",
    "print(f\"{'Sample':<10} {'i+1 SSIM':<12} {'i+2 SSIM':<12} {'i+3 SSIM':<12} {'i+1 PSNR':<12} {'i+2 PSNR':<12} {'i+3 PSNR':<12}\")\n",
    "print(f\"{'-'*85}\")\n",
    "\n",
    "for i in range(min(10, len(ssim_i1))):\n",
    "    print(f\"{i+1:<10} {ssim_i1[i]:<12.4f} {ssim_i2[i]:<12.4f} {ssim_i3[i]:<12.4f} \"\n",
    "          f\"{psnr_i1[i]:<12.2f} {psnr_i2[i]:<12.2f} {psnr_i3[i]:<12.2f}\")\n",
    "\n",
    "if len(ssim_i1) > 10:\n",
    "    print(f\"... and {len(ssim_i1)-10} more samples\")\n",
    "\n",
    "print(\"=\"*90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60f003",
   "metadata": {},
   "source": [
    "# Visualization and Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32fac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Total loss\n",
    "axes[0, 0].plot(train_losses['total'], linewidth=2, marker='o', markersize=3, label='Train')\n",
    "axes[0, 0].set_title('Total Loss (All Stages)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# i+1 Loss\n",
    "axes[0, 1].plot(train_losses['i1'], linewidth=2, marker='o', markersize=3, label='i+1', color='orange')\n",
    "axes[0, 1].set_title('i+1 Prediction Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# i+2 Loss\n",
    "axes[1, 0].plot(train_losses['i2'], linewidth=2, marker='o', markersize=3, label='i+2', color='green')\n",
    "axes[1, 0].set_title('i+2 Prediction Loss (Stage 1)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# i+3 Loss\n",
    "axes[1, 1].plot(train_losses['i3'], linewidth=2, marker='o', markersize=3, label='i+3', color='red')\n",
    "axes[1, 1].set_title('i+3 Prediction Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_SAVE_DIR / 'progressive_unet_training_curves.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to {plot_path}\")\n",
    "\n",
    "# Save training history and metrics\n",
    "history = {\n",
    "    'train_losses': train_losses,\n",
    "    'test_metrics': {\n",
    "        'i1': {'ssim_mean': float(ssim_i1.mean()), 'ssim_std': float(ssim_i1.std()),\n",
    "               'psnr_mean': float(psnr_i1.mean()), 'psnr_std': float(psnr_i1.std())},\n",
    "        'i2': {'ssim_mean': float(ssim_i2.mean()), 'ssim_std': float(ssim_i2.std()),\n",
    "               'psnr_mean': float(psnr_i2.mean()), 'psnr_std': float(psnr_i2.std())},\n",
    "        'i3': {'ssim_mean': float(ssim_i3.mean()), 'ssim_std': float(ssim_i3.std()),\n",
    "               'psnr_mean': float(psnr_i3.mean()), 'psnr_std': float(psnr_i3.std())},\n",
    "        'average': {'ssim': float(avg_ssim), 'psnr': float(avg_psnr)}\n",
    "    },\n",
    "    'config': {\n",
    "        'epochs': len(train_losses['total']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'loss_weights': LOSS_WEIGHTS,\n",
    "        'architecture': 'Progressive UNet (3-stage)',\n",
    "        'stages': {\n",
    "            'stage1': 'UNet(i, i+4) -> i+2',\n",
    "            'stage2a': 'UNet(i, i+2_gen) -> i+1',\n",
    "            'stage2b': 'UNet(i+2_gen, i+4) -> i+3'\n",
    "        }\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "log_path = RESULTS_SAVE_DIR / 'progressive_unet_history.json'\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Training history saved to {log_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*90)\n",
    "print(\"PROGRESSIVE UNet TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*90)\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"   - Stage 1 (UNet1): Predicts i+2 from (i, i+4)\")\n",
    "print(f\"   - Stage 2A (UNet2): Predicts i+1 from (i, i+2_generated)\")\n",
    "print(f\"   - Stage 2B (UNet3): Predicts i+3 from (i+2_generated, i+4)\")\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"   Epochs trained: {len(train_losses['total'])}\")\n",
    "print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"   i+1 - SSIM: {ssim_i1.mean():.4f} ¬± {ssim_i1.std():.4f}, PSNR: {psnr_i1.mean():.2f} ¬± {psnr_i1.std():.2f} dB\")\n",
    "print(f\"   i+2 - SSIM: {ssim_i2.mean():.4f} ¬± {ssim_i2.std():.4f}, PSNR: {psnr_i2.mean():.2f} ¬± {psnr_i2.std():.2f} dB\")\n",
    "print(f\"   i+3 - SSIM: {ssim_i3.mean():.4f} ¬± {ssim_i3.std():.4f}, PSNR: {psnr_i3.mean():.2f} ¬± {psnr_i3.std():.2f} dB\")\n",
    "print(f\"   Average - SSIM: {avg_ssim:.4f}, PSNR: {avg_psnr:.2f} dB\")\n",
    "print(f\"\\nLoss Weights (Multi-Scale):\")\n",
    "print(f\"   w_i1 (i+1 prediction): {LOSS_WEIGHTS['w_i1']}\")\n",
    "print(f\"   w_i2 (i+2 prediction): {LOSS_WEIGHTS['w_i2']}\")\n",
    "print(f\"   w_i3 (i+3 prediction): {LOSS_WEIGHTS['w_i3']}\")\n",
    "print(f\"\\nFiles saved to: {MODEL_SAVE_DIR}\")\n",
    "print(\"=\"*90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7335b9-5764-4d0d-bd31-ef478027ec7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce0adb-df24-4396-bc23-c71945659efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmia-2",
   "language": "python",
   "name": "dlmia-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
