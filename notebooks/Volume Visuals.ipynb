{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14e6fca-de8d-417d-8d32-ccd50a18c4f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98581cfc-8404-4859-a643-ee769a06bd30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add src to path and import data generator\n",
    "sys.path.insert(0, '../src')\n",
    "from VolumeVisualization import predict_volume_and_visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9119f2fc-70a2-4a4a-9bf8-544c9a7051f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MULTI-MODEL VOLUME PREDICTION & VISUALIZATION\n",
      "======================================================================\n",
      "\n",
      "1. Loading random patient from test set...\n",
      "   Patient: Prostate-MRI-US-Biopsy-0739\n",
      "   Volume shape: (60, 256, 256)\n",
      "   Triplets: 29\n",
      "\n",
      "2. Running inference with all models...\n",
      "\n",
      "   ⏳ Processing UNET...\n",
      "/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/models\n",
      "/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/models/unet_best.pt\n",
      "✓ Loaded UNET model from unet_best.pt\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "      ✓ UNET prediction complete\n",
      "\n",
      "   ⏳ Processing DEEPCNN...\n",
      "/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/models\n",
      "/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/models/deepcnn_best.pt\n",
      "✓ Loaded DEEPCNN model from deepcnn_best.pt\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([4, 1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "      ✓ DEEPCNN prediction complete\n",
      "\n",
      "   ⏳ Processing PROGRESSIVE_UNET...\n",
      "/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/models\n",
      "/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/models/progressive_unet_best.pt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ProgressiveUNet.__init__() got an unexpected keyword argument 'in_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m pred_path = results_dir  + \u001b[33m'\u001b[39m\u001b[33m/volume_visualization_all_except_ddpm.png\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#plt.savefig(pred_path, dpi=150, bbox_inches='tight')\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mpredict_volume_and_visualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/notebooks/../src/VolumeVisualization.py:346\u001b[39m, in \u001b[36mpredict_volume_and_visualize\u001b[39m\u001b[34m(seed, device, batch_size, save_path, parallel_viz)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m   ⏳ Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m      ⚠️  Skipped: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/notebooks/../src/VolumeVisualization.py:179\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_name, device)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03mLoad the best model checkpoint for the given model name.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03mUses correct model architectures from ModelLoader.py\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m \u001b[33;03m    ValueError: If model_name not recognized or checkpoint not found\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModelLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model \u001b[38;5;28;01mas\u001b[39;00m loader_load_model\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/eel6935/dthiyagarajan/Multi-Image-Super-Resolution/notebooks/../src/ModelLoader.py:487\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_name, device)\u001b[39m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# Initialize and load model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m model = \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m    488\u001b[39m checkpoint = torch.load(checkpoint_path, map_location=device)\n\u001b[32m    489\u001b[39m model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: ProgressiveUNet.__init__() got an unexpected keyword argument 'in_channels'"
     ]
    }
   ],
   "source": [
    "results_dir = '../results'\n",
    "pred_path = results_dir  + '/volume_visualization_all_except_ddpm.png'\n",
    "#plt.savefig(pred_path, dpi=150, bbox_inches='tight')\n",
    "\n",
    "predict_volume_and_visualize(batch_size = 4, save_path = pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b4feb-a715-4726-9a62-87b9298086a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmia-2",
   "language": "python",
   "name": "dlmia-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
