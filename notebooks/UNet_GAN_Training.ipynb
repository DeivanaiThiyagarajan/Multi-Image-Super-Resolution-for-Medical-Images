{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84dd8440",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99440d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device: cuda\n",
      "   GPU: NVIDIA B200\n",
      "   Memory: 191.51 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Check device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"‚úÖ Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01d4900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup path to access custom modules\n",
    "root_directory = os.path.dirname(os.getcwd())\n",
    "sys.path.append(os.path.join(root_directory, 'src'))\n",
    "\n",
    "from ModelDataGenerator import build_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49401f1e",
   "metadata": {},
   "source": [
    "# GPU Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dcb3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Memory Usage:\n",
      "   Allocated: 0.00 GB\n",
      "   Reserved: 0.00 GB\n",
      "   Total Available: 191.51 GB\n",
      "‚úÖ GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU cache and garbage collect\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"‚úÖ GPU memory cleared\")\n",
    "\n",
    "# Check initial GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    initial_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved_memory = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"Initial GPU Memory Usage:\")\n",
    "    print(f\"   Allocated: {initial_memory:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved_memory:.2f} GB\")\n",
    "    print(f\"   Total Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df7d326",
   "metadata": {},
   "source": [
    "# Generator: UNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d555cb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ UNet Generator defined\n"
     ]
    }
   ],
   "source": [
    "class UNetBlock(nn.Module):\n",
    "    \"\"\"Basic UNet encoder/decoder block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(UNetBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet Generator for medical image super-resolution\n",
    "    Used as generator in GAN framework\n",
    "    \n",
    "    Input: (B, 2, H, W) - prior and posterior slices\n",
    "    Output: (B, 1, H, W) - super-resolved middle slice\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=2, out_channels=1, base_features=64):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = UNetBlock(in_channels, base_features)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc2 = UNetBlock(base_features, base_features * 2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc3 = UNetBlock(base_features * 2, base_features * 4)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc4 = UNetBlock(base_features * 4, base_features * 8)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNetBlock(base_features * 8, base_features * 16)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(base_features * 16, base_features * 8, kernel_size=2, stride=2)\n",
    "        self.dec4 = UNetBlock(base_features * 16, base_features * 8)\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(base_features * 8, base_features * 4, kernel_size=2, stride=2)\n",
    "        self.dec3 = UNetBlock(base_features * 8, base_features * 4)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(base_features * 4, base_features * 2, kernel_size=2, stride=2)\n",
    "        self.dec2 = UNetBlock(base_features * 4, base_features * 2)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(base_features * 2, base_features, kernel_size=2, stride=2)\n",
    "        self.dec1 = UNetBlock(base_features * 2, base_features)\n",
    "        \n",
    "        # Final output\n",
    "        self.final = nn.Conv2d(base_features, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        x = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.enc2(x)\n",
    "        x = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.enc3(x)\n",
    "        x = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.enc4(x)\n",
    "        x = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.upconv4(x)\n",
    "        x = torch.cat([x, enc4], dim=1)\n",
    "        x = self.dec4(x)\n",
    "        \n",
    "        x = self.upconv3(x)\n",
    "        x = torch.cat([x, enc3], dim=1)\n",
    "        x = self.dec3(x)\n",
    "        \n",
    "        x = self.upconv2(x)\n",
    "        x = torch.cat([x, enc2], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        \n",
    "        x = self.upconv1(x)\n",
    "        x = torch.cat([x, enc1], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ UNet Generator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452eeb5",
   "metadata": {},
   "source": [
    "# Discriminator: PatchGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3019b5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PatchGAN Discriminator defined\n"
     ]
    }
   ],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch-based Discriminator (PatchGAN style)\n",
    "    Discriminates between real and generated images at patch level\n",
    "    \n",
    "    Input: (B, 1, H, W) - single channel image\n",
    "    Output: (B, 1, H/16, W/16) - patch-wise validity scores\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, base_features=64):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        \n",
    "        def conv_block(in_ch, out_ch, stride=1, use_bn=True):\n",
    "            layers = [nn.Conv2d(in_ch, out_ch, kernel_size=4, stride=stride, padding=1)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm2d(out_ch))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        # PatchGAN discriminator\n",
    "        self.conv1 = conv_block(in_channels, base_features, stride=2, use_bn=False)  # 128x128\n",
    "        self.conv2 = conv_block(base_features, base_features * 2, stride=2)           # 64x64\n",
    "        self.conv3 = conv_block(base_features * 2, base_features * 4, stride=2)       # 32x32\n",
    "        self.conv4 = conv_block(base_features * 4, base_features * 8, stride=2)       # 16x16\n",
    "        self.conv5 = nn.Conv2d(base_features * 8, 1, kernel_size=4, stride=1, padding=1)  # 16x16 patches\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ PatchGAN Discriminator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c7e61",
   "metadata": {},
   "source": [
    "# Loss Functions for GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7686e47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Perceptual Loss defined\n"
     ]
    }
   ],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual loss using VGG16 features\"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        \n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg.features.children())[:31]).to(device)\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Convert single-channel to 3-channel for VGG16\n",
    "        pred_3ch = torch.cat([pred, pred, pred], dim=1)\n",
    "        target_3ch = torch.cat([target, target, target], dim=1)\n",
    "        \n",
    "        # Extract features\n",
    "        pred_features = self.feature_extractor(pred_3ch)\n",
    "        target_features = self.feature_extractor(target_3ch)\n",
    "        \n",
    "        # L1 loss on features\n",
    "        loss = F.l1_loss(pred_features, target_features)\n",
    "        \n",
    "        # Cleanup\n",
    "        del pred_3ch, target_3ch, pred_features, target_features\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"‚úÖ Perceptual Loss defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ea19e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GAN Loss defined\n"
     ]
    }
   ],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"GAN Loss - Adversarial loss for generator and discriminator\"\"\"\n",
    "    def __init__(self, use_lsgan=True):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.use_lsgan = use_lsgan\n",
    "    \n",
    "    def discriminator_loss(self, discriminator_pred_real, discriminator_pred_fake):\n",
    "        \"\"\"\n",
    "        Discriminator loss: maximize D(real) and minimize D(fake)\n",
    "        For LSGan: D_loss = (D(real) - 1)^2 + D(fake)^2\n",
    "        For BCE: D_loss = -log(D(real)) - log(1 - D(fake))\n",
    "        \"\"\"\n",
    "        if self.use_lsgan:\n",
    "            # Least Squares GAN (more stable)\n",
    "            loss_real = torch.mean((discriminator_pred_real - 1) ** 2)\n",
    "            loss_fake = torch.mean(discriminator_pred_fake ** 2)\n",
    "        else:\n",
    "            # Binary Cross Entropy GAN\n",
    "            ones = torch.ones_like(discriminator_pred_real)\n",
    "            zeros = torch.zeros_like(discriminator_pred_fake)\n",
    "            loss_real = F.binary_cross_entropy_with_logits(discriminator_pred_real, ones)\n",
    "            loss_fake = F.binary_cross_entropy_with_logits(discriminator_pred_fake, zeros)\n",
    "        \n",
    "        return loss_real + loss_fake\n",
    "    \n",
    "    def generator_loss(self, discriminator_pred_fake):\n",
    "        \"\"\"\n",
    "        Generator loss: fool the discriminator\n",
    "        For LSGan: G_loss = (D(G(z)) - 1)^2\n",
    "        For BCE: G_loss = -log(D(G(z)))\n",
    "        \"\"\"\n",
    "        if self.use_lsgan:\n",
    "            # Least Squares GAN\n",
    "            loss = torch.mean((discriminator_pred_fake - 1) ** 2)\n",
    "        else:\n",
    "            # Binary Cross Entropy GAN\n",
    "            ones = torch.ones_like(discriminator_pred_fake)\n",
    "            loss = F.binary_cross_entropy_with_logits(discriminator_pred_fake, ones)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"‚úÖ GAN Loss defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec2e4f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9ce040-00e0-4b57-8504-1ec316fc9086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b034dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° DATA CONFIGURATION:\n",
      "   BATCH_SIZE: 4\n",
      "   NUM_WORKERS: 4\n",
      "   AUGMENTATION: True\n",
      "   Note: Reduced batch size for GAN (generator + discriminator)\n",
      "\n",
      "‚úÖ Data loaders created\n",
      "   Train: 18269 batches\n",
      "   Val: 3221 batches\n",
      "   Test: 4560 batches\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 4  # Reduced for GAN training (both G and D)\n",
    "NUM_WORKERS = 4\n",
    "AUGMENT = True\n",
    "\n",
    "print(f\"‚ö° DATA CONFIGURATION:\")\n",
    "print(f\"   BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"   NUM_WORKERS: {NUM_WORKERS}\")\n",
    "print(f\"   AUGMENTATION: {AUGMENT}\")\n",
    "print(f\"   Note: Reduced batch size for GAN (generator + discriminator)\")\n",
    "\n",
    "# Build dataloaders\n",
    "train_loader = build_dataloader(\n",
    "    split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=AUGMENT,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = build_dataloader(\n",
    "    split=\"val\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = build_dataloader(\n",
    "    split=\"test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    augment=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaders created\")\n",
    "print(f\"   Train: {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_loader)} batches\")\n",
    "print(f\"   Test: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc5c58",
   "metadata": {},
   "source": [
    "# Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c14766",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "   Epochs: 20\n",
      "   Generator LR: 0.0002\n",
      "   Discriminator LR: 0.0002\n",
      "   Early stopping patience: 5\n",
      "   Batch size: 4\n",
      "\n",
      "Loss Weights:\n",
      "   L1 (Reconstruction): 1.0\n",
      "   Perceptual: 0.1\n",
      "   Adversarial: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/blue/cruzcastrol/dthiyagarajan/.conda/envs/dlmia-2/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Models initialized on cuda\n",
      "   Generator parameters: 31,037,057\n",
      "   Discriminator parameters: 2,764,481\n"
     ]
    }
   ],
   "source": [
    "# Training Configuration\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE_G = 2e-4  # Generator learning rate\n",
    "LEARNING_RATE_D = 2e-4  # Discriminator learning rate\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "MODEL_SAVE_DIR = Path('../models')\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_SAVE_DIR = Path('../results')\n",
    "RESULTS_SAVE_DIR.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Loss weights (based on miSRGAN paper)\n",
    "LAMBDA_L1 = 1.0          # Reconstruction loss (MSE/L1)\n",
    "LAMBDA_PERCEPTUAL = 0.1  # Perceptual loss weight\n",
    "LAMBDA_ADVERSARIAL = 0.01 # Adversarial loss weight\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Generator LR: {LEARNING_RATE_G}\")\n",
    "print(f\"   Discriminator LR: {LEARNING_RATE_D}\")\n",
    "print(f\"   Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nLoss Weights:\")\n",
    "print(f\"   L1 (Reconstruction): {LAMBDA_L1}\")\n",
    "print(f\"   Perceptual: {LAMBDA_PERCEPTUAL}\")\n",
    "print(f\"   Adversarial: {LAMBDA_ADVERSARIAL}\")\n",
    "\n",
    "# Initialize models\n",
    "generator = UNetGenerator(in_channels=2, out_channels=1, base_features=64).to(DEVICE)\n",
    "discriminator = PatchDiscriminator(in_channels=1, base_features=64).to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=LEARNING_RATE_G, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_D, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss functions\n",
    "mse_loss = nn.MSELoss()\n",
    "perceptual_loss = PerceptualLoss(device=DEVICE)\n",
    "gan_loss = GANLoss(use_lsgan=True)  # Least Squares GAN for stability\n",
    "\n",
    "print(f\"\\n‚úÖ Models initialized on {DEVICE}\")\n",
    "print(f\"   Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"   Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b811e662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint utilities loaded\n",
      "\n",
      "üîç Checking for existing checkpoints...\n",
      "üìÇ Found checkpoint: unet_gan_checkpoint_16.pt\n",
      "‚úÖ Loaded checkpoint from epoch 16\n",
      "   Resuming training from epoch 17\n",
      "   Previous epochs completed: 16\n"
     ]
    }
   ],
   "source": [
    "# ========== Checkpoint Utilities ==========\n",
    "def get_latest_checkpoint(checkpoint_dir, prefix='unet_gan_checkpoint'):\n",
    "    \"\"\"Get the latest checkpoint file by epoch number\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_files = list(checkpoint_dir.glob(f'{prefix}_*.pt'))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Extract epoch numbers and sort\n",
    "    checkpoints_with_epochs = []\n",
    "    for ckpt in checkpoint_files:\n",
    "        try:\n",
    "            epoch = int(ckpt.stem.split('_')[-1])\n",
    "            checkpoints_with_epochs.append((epoch, ckpt))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    if not checkpoints_with_epochs:\n",
    "        return None\n",
    "    \n",
    "    # Return path of checkpoint with highest epoch\n",
    "    latest_epoch, latest_ckpt = max(checkpoints_with_epochs, key=lambda x: x[0])\n",
    "    return latest_ckpt, latest_epoch\n",
    "\n",
    "\n",
    "def load_checkpoint(model_g, model_d, optimizer_g, optimizer_d, checkpoint_path, device):\n",
    "    \"\"\"Load checkpoint and return starting epoch\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model_g.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    model_d.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
    "    optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "    \n",
    "    return start_epoch, checkpoint\n",
    "\n",
    "\n",
    "def save_checkpoint(model_g, model_d, optimizer_g, optimizer_d, epoch, train_losses, val_losses, checkpoint_path):\n",
    "    \"\"\"Save checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': model_g.state_dict(),\n",
    "        'discriminator_state_dict': model_d.state_dict(),\n",
    "        'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "        'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "print(\"‚úÖ Checkpoint utilities loaded\")\n",
    "\n",
    "# ========== Check for Existing Checkpoint ==========\n",
    "print(\"\\nüîç Checking for existing checkpoints...\")\n",
    "latest_ckpt_info = get_latest_checkpoint(MODEL_SAVE_DIR, prefix='unet_gan_checkpoint')\n",
    "\n",
    "if latest_ckpt_info is not None:\n",
    "    latest_ckpt_path, latest_epoch = latest_ckpt_info\n",
    "    print(f\"üìÇ Found checkpoint: {latest_ckpt_path.name}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    start_epoch, loaded_ckpt = load_checkpoint(\n",
    "        generator, discriminator, optimizer_g, optimizer_d, \n",
    "        latest_ckpt_path, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Restore training state\n",
    "    train_losses = loaded_ckpt.get('train_losses', {'g': [], 'd': [], 'l1': [], 'perc': [], 'adv': []})\n",
    "    val_losses_saved = loaded_ckpt.get('val_losses', {})\n",
    "    \n",
    "    print(f\"‚úÖ Loaded checkpoint from epoch {latest_epoch}\")\n",
    "    print(f\"   Resuming training from epoch {start_epoch}\")\n",
    "    print(f\"   Previous epochs completed: {latest_epoch}\")\n",
    "else:\n",
    "    print(\"üì≠ No checkpoint found - starting fresh training\")\n",
    "    start_epoch = 1\n",
    "    train_losses = {'g': [], 'd': [], 'l1': [], 'perc': [], 'adv': []}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d96d33",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e8c0ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training function defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(generator, discriminator, train_loader, optimizer_g, optimizer_d, \n",
    "                mse_loss, perceptual_loss, gan_loss, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    total_g_loss = 0.0\n",
    "    total_d_loss = 0.0\n",
    "    total_l1_loss = 0.0\n",
    "    total_perc_loss = 0.0\n",
    "    total_adv_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for (pre, post), target in pbar:\n",
    "        # Stack prior and posterior as input\n",
    "        inputs = torch.cat([pre, post], dim=1).to(device)\n",
    "        targets = target.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        \n",
    "        # ========== Generator Training ==========\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        # Generate predictions\n",
    "        generated = generator(inputs)\n",
    "        \n",
    "        # L1/MSE loss\n",
    "        loss_l1 = mse_loss(generated, targets)\n",
    "        \n",
    "        # Perceptual loss\n",
    "        loss_perc = perceptual_loss(generated, targets)\n",
    "        \n",
    "        # Discriminator prediction on generated images\n",
    "        d_pred_fake = discriminator(generated.detach())\n",
    "        \n",
    "        # Adversarial loss (fool discriminator)\n",
    "        loss_adv = gan_loss.generator_loss(d_pred_fake)\n",
    "        \n",
    "        # Total generator loss\n",
    "        loss_g = LAMBDA_L1 * loss_l1 + LAMBDA_PERCEPTUAL * loss_perc + LAMBDA_ADVERSARIAL * loss_adv\n",
    "        \n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # ========== Discriminator Training ==========\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        # Real images\n",
    "        d_pred_real = discriminator(targets)\n",
    "        \n",
    "        # Fake images (no gradient)\n",
    "        generated_detached = generator(inputs).detach()\n",
    "        d_pred_fake = discriminator(generated_detached)\n",
    "        \n",
    "        # Discriminator loss\n",
    "        loss_d = gan_loss.discriminator_loss(d_pred_real, d_pred_fake)\n",
    "        \n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        total_g_loss += loss_g.item()\n",
    "        total_d_loss += loss_d.item()\n",
    "        total_l1_loss += loss_l1.item()\n",
    "        total_perc_loss += loss_perc.item()\n",
    "        total_adv_loss += loss_adv.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'G_loss': f'{loss_g.item():.4f}',\n",
    "            'D_loss': f'{loss_d.item():.4f}'\n",
    "        })\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        del inputs, targets, generated, generated_detached, d_pred_real, d_pred_fake\n",
    "        del loss_l1, loss_perc, loss_adv, loss_g, loss_d\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'g_loss': total_g_loss / len(train_loader),\n",
    "        'd_loss': total_d_loss / len(train_loader),\n",
    "        'l1_loss': total_l1_loss / len(train_loader),\n",
    "        'perc_loss': total_perc_loss / len(train_loader),\n",
    "        'adv_loss': total_adv_loss / len(train_loader)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49cc2d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation function defined\n"
     ]
    }
   ],
   "source": [
    "def validate(generator, discriminator, val_loader, mse_loss, perceptual_loss, gan_loss, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    total_g_loss = 0.0\n",
    "    total_d_loss = 0.0\n",
    "    total_l1_loss = 0.0\n",
    "    total_perc_loss = 0.0\n",
    "    total_adv_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for (pre, post), target in pbar:\n",
    "            inputs = torch.cat([pre, post], dim=1).to(device)\n",
    "            targets = target.to(device)\n",
    "            \n",
    "            # Generator forward\n",
    "            generated = generator(inputs)\n",
    "            \n",
    "            # Losses\n",
    "            loss_l1 = mse_loss(generated, targets)\n",
    "            loss_perc = perceptual_loss(generated, targets)\n",
    "            d_pred_fake = discriminator(generated)\n",
    "            d_pred_real = discriminator(targets)\n",
    "            loss_adv = gan_loss.generator_loss(d_pred_fake)\n",
    "            loss_g = LAMBDA_L1 * loss_l1 + LAMBDA_PERCEPTUAL * loss_perc + LAMBDA_ADVERSARIAL * loss_adv\n",
    "            loss_d = gan_loss.discriminator_loss(d_pred_real, d_pred_fake)\n",
    "            \n",
    "            total_g_loss += loss_g.item()\n",
    "            total_d_loss += loss_d.item()\n",
    "            total_l1_loss += loss_l1.item()\n",
    "            total_perc_loss += loss_perc.item()\n",
    "            total_adv_loss += loss_adv.item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'G_loss': f'{loss_g.item():.4f}',\n",
    "                'D_loss': f'{loss_d.item():.4f}'\n",
    "            })\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            del inputs, targets, generated, d_pred_fake, d_pred_real\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'g_loss': total_g_loss / len(val_loader),\n",
    "        'd_loss': total_d_loss / len(val_loader),\n",
    "        'l1_loss': total_l1_loss / len(val_loader),\n",
    "        'perc_loss': total_perc_loss / len(val_loader),\n",
    "        'adv_loss': total_adv_loss / len(val_loader)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Validation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a84be1-a59f-423a-b2ab-632ee0cb92d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Epoch 3-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb23bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best validation loss from checkpoint: 0.1076\n",
      "\n",
      "Starting training...\n",
      "\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/10 | G_Loss: 0.0933 | D_Loss: 0.0296 | L1: 0.0659 | Perc: 0.1767 | Adv: 0.9649 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/10 | G_Loss: 0.0914 | D_Loss: 0.0243 | L1: 0.0641 | Perc: 0.1752 | Adv: 0.9708 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 8161/18269 [1:14:31<1:26:34,  1.95it/s, G_loss=0.0854, D_loss=0.0018]"
     ]
    }
   ],
   "source": [
    "# Initialize training variables (may be restored from checkpoint above)\n",
    "if 'train_losses' not in locals():\n",
    "    train_losses = {'g': [], 'd': [], 'l1': [], 'perc': [], 'adv': []}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Calculate best_val_loss from history if resuming\n",
    "if len(train_losses['g']) > 0:\n",
    "    # Find minimum validation loss from saved history\n",
    "    if 'val_losses_saved' in locals() and val_losses_saved:\n",
    "        best_val_loss = val_losses_saved.get('g_loss', float('inf'))\n",
    "        print(f\"üìä Best validation loss from checkpoint: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss_dict = train_epoch(generator, discriminator, train_loader, \n",
    "                                  optimizer_g, optimizer_d, mse_loss, perceptual_loss, \n",
    "                                  gan_loss, DEVICE)\n",
    "    \n",
    "    val_loss_dict = validate(generator, discriminator, val_loader, \n",
    "                             mse_loss, perceptual_loss, gan_loss, DEVICE)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses['g'].append(train_loss_dict['g_loss'])\n",
    "    train_losses['d'].append(train_loss_dict['d_loss'])\n",
    "    train_losses['l1'].append(train_loss_dict['l1_loss'])\n",
    "    train_losses['perc'].append(train_loss_dict['perc_loss'])\n",
    "    train_losses['adv'].append(train_loss_dict['adv_loss'])\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"G_Loss: {train_loss_dict['g_loss']:.4f} | \"\n",
    "              f\"D_Loss: {train_loss_dict['d_loss']:.4f} | \"\n",
    "              f\"L1: {train_loss_dict['l1_loss']:.4f} | \"\n",
    "              f\"Perc: {train_loss_dict['perc_loss']:.4f} | \"\n",
    "              f\"Adv: {train_loss_dict['adv_loss']:.4f}\", end=\"\")\n",
    "        \n",
    "        val_g_loss = val_loss_dict['g_loss']\n",
    "        \n",
    "        # Save checkpoint at every epoch\n",
    "        ckpt_path = MODEL_SAVE_DIR / f'unet_gan_checkpoint_{epoch}.pt'\n",
    "        save_checkpoint(generator, discriminator, optimizer_g, optimizer_d, \n",
    "                       epoch, train_losses, val_loss_dict, ckpt_path)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_g_loss < best_val_loss:\n",
    "            best_val_loss = val_g_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best models\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "                'val_loss': val_g_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_loss_dict\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_SAVE_DIR / 'unet_gan_best.pt')\n",
    "            print(\" (Best)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "            \n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "                break\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90034f19-53b3-4c90-b71a-205369d79ba0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Epoch 4-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435f8be-f2bc-4b39-ab02-702ce2fdabff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best validation loss from checkpoint: 0.1035\n",
      "\n",
      "Starting training...\n",
      "\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/20 | G_Loss: 0.0900 | D_Loss: 0.0230 | L1: 0.0629 | Perc: 0.1740 | Adv: 0.9722 (patience: 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/20 | G_Loss: 0.0890 | D_Loss: 0.0217 | L1: 0.0619 | Perc: 0.1731 | Adv: 0.9729 (patience: 2/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7/20 | G_Loss: 0.0881 | D_Loss: 0.0196 | L1: 0.0611 | Perc: 0.1724 | Adv: 0.9752 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/20 | G_Loss: 0.0875 | D_Loss: 0.0185 | L1: 0.0605 | Perc: 0.1718 | Adv: 0.9767 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 9364/18269 [1:10:48<1:13:07,  2.03it/s, G_loss=0.0911, D_loss=0.0014]"
     ]
    }
   ],
   "source": [
    "# Initialize training variables (may be restored from checkpoint above)\n",
    "if 'train_losses' not in locals():\n",
    "    train_losses = {'g': [], 'd': [], 'l1': [], 'perc': [], 'adv': []}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Calculate best_val_loss from history if resuming\n",
    "if len(train_losses['g']) > 0:\n",
    "    # Find minimum validation loss from saved history\n",
    "    if 'val_losses_saved' in locals() and val_losses_saved:\n",
    "        best_val_loss = val_losses_saved.get('g_loss', float('inf'))\n",
    "        print(f\"üìä Best validation loss from checkpoint: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss_dict = train_epoch(generator, discriminator, train_loader, \n",
    "                                  optimizer_g, optimizer_d, mse_loss, perceptual_loss, \n",
    "                                  gan_loss, DEVICE)\n",
    "    \n",
    "    val_loss_dict = validate(generator, discriminator, val_loader, \n",
    "                             mse_loss, perceptual_loss, gan_loss, DEVICE)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses['g'].append(train_loss_dict['g_loss'])\n",
    "    train_losses['d'].append(train_loss_dict['d_loss'])\n",
    "    train_losses['l1'].append(train_loss_dict['l1_loss'])\n",
    "    train_losses['perc'].append(train_loss_dict['perc_loss'])\n",
    "    train_losses['adv'].append(train_loss_dict['adv_loss'])\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"G_Loss: {train_loss_dict['g_loss']:.4f} | \"\n",
    "              f\"D_Loss: {train_loss_dict['d_loss']:.4f} | \"\n",
    "              f\"L1: {train_loss_dict['l1_loss']:.4f} | \"\n",
    "              f\"Perc: {train_loss_dict['perc_loss']:.4f} | \"\n",
    "              f\"Adv: {train_loss_dict['adv_loss']:.4f}\", end=\"\")\n",
    "        \n",
    "        val_g_loss = val_loss_dict['g_loss']\n",
    "        \n",
    "        # Save checkpoint at every epoch\n",
    "        ckpt_path = MODEL_SAVE_DIR / f'unet_gan_checkpoint_{epoch}.pt'\n",
    "        save_checkpoint(generator, discriminator, optimizer_g, optimizer_d, \n",
    "                       epoch, train_losses, val_loss_dict, ckpt_path)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_g_loss < best_val_loss:\n",
    "            best_val_loss = val_g_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best models\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "                'val_loss': val_g_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_loss_dict\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_SAVE_DIR / 'unet_gan_best.pt')\n",
    "            print(\" (Best)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "            \n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "                break\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b69bb-8fc1-40d8-bf46-a450012252d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Epoch 9-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8204c-4629-4c00-83a3-b171ef19bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best validation loss from checkpoint: 0.1022\n",
      "\n",
      "Starting training...\n",
      "\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9/20 | G_Loss: 0.0868 | D_Loss: 0.0170 | L1: 0.0599 | Perc: 0.1713 | Adv: 0.9787 (patience: 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/20 | G_Loss: 0.0863 | D_Loss: 0.0159 | L1: 0.0595 | Perc: 0.1708 | Adv: 0.9796 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11/20 | G_Loss: 0.0858 | D_Loss: 0.0154 | L1: 0.0590 | Perc: 0.1704 | Adv: 0.9802 (patience: 1/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12/20 | G_Loss: 0.0854 | D_Loss: 0.0152 | L1: 0.0586 | Perc: 0.1700 | Adv: 0.9805 (patience: 2/5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 7866/18269 [1:02:03<1:30:30,  1.92it/s, G_loss=0.0515, D_loss=0.0205]"
     ]
    }
   ],
   "source": [
    "# Initialize training variables (may be restored from checkpoint above)\n",
    "if 'train_losses' not in locals():\n",
    "    train_losses = {'g': [], 'd': [], 'l1': [], 'perc': [], 'adv': []}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Calculate best_val_loss from history if resuming\n",
    "if len(train_losses['g']) > 0:\n",
    "    # Find minimum validation loss from saved history\n",
    "    if 'val_losses_saved' in locals() and val_losses_saved:\n",
    "        best_val_loss = val_losses_saved.get('g_loss', float('inf'))\n",
    "        print(f\"üìä Best validation loss from checkpoint: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss_dict = train_epoch(generator, discriminator, train_loader, \n",
    "                                  optimizer_g, optimizer_d, mse_loss, perceptual_loss, \n",
    "                                  gan_loss, DEVICE)\n",
    "    \n",
    "    val_loss_dict = validate(generator, discriminator, val_loader, \n",
    "                             mse_loss, perceptual_loss, gan_loss, DEVICE)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses['g'].append(train_loss_dict['g_loss'])\n",
    "    train_losses['d'].append(train_loss_dict['d_loss'])\n",
    "    train_losses['l1'].append(train_loss_dict['l1_loss'])\n",
    "    train_losses['perc'].append(train_loss_dict['perc_loss'])\n",
    "    train_losses['adv'].append(train_loss_dict['adv_loss'])\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"G_Loss: {train_loss_dict['g_loss']:.4f} | \"\n",
    "              f\"D_Loss: {train_loss_dict['d_loss']:.4f} | \"\n",
    "              f\"L1: {train_loss_dict['l1_loss']:.4f} | \"\n",
    "              f\"Perc: {train_loss_dict['perc_loss']:.4f} | \"\n",
    "              f\"Adv: {train_loss_dict['adv_loss']:.4f}\", end=\"\")\n",
    "        \n",
    "        val_g_loss = val_loss_dict['g_loss']\n",
    "        \n",
    "        # Save checkpoint at every epoch\n",
    "        ckpt_path = MODEL_SAVE_DIR / f'unet_gan_checkpoint_{epoch}.pt'\n",
    "        save_checkpoint(generator, discriminator, optimizer_g, optimizer_d, \n",
    "                       epoch, train_losses, val_loss_dict, ckpt_path)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_g_loss < best_val_loss:\n",
    "            best_val_loss = val_g_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best models\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "                'val_loss': val_g_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_loss_dict\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_SAVE_DIR / 'unet_gan_best.pt')\n",
    "            print(\" (Best)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "            \n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "                break\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9579a-8517-46a2-947e-19101b357889",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Epoch 13-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7705861b-d392-4465-ab96-a9dbf6d2b2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e39ba1-a9d7-48f2-8dde-abbdd281edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best validation loss from checkpoint: 0.1008\n",
      "\n",
      "Starting training...\n",
      "\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13/20 | G_Loss: 0.0851 | D_Loss: 0.0125 | L1: 0.0583 | Perc: 0.1697 | Adv: 0.9840 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|‚ñà‚ñà‚ñè       | 3956/18269 [29:22<1:44:52,  2.27it/s, G_loss=0.0916, D_loss=0.0019]"
     ]
    }
   ],
   "source": [
    "# Initialize training variables (may be restored from checkpoint above)\n",
    "if 'train_losses' not in locals():\n",
    "    train_losses = {'g': [], 'd': [], 'l1': [], 'perc': [], 'adv': []}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Calculate best_val_loss from history if resuming\n",
    "if len(train_losses['g']) > 0:\n",
    "    # Find minimum validation loss from saved history\n",
    "    if 'val_losses_saved' in locals() and val_losses_saved:\n",
    "        best_val_loss = val_losses_saved.get('g_loss', float('inf'))\n",
    "        print(f\"üìä Best validation loss from checkpoint: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss_dict = train_epoch(generator, discriminator, train_loader, \n",
    "                                  optimizer_g, optimizer_d, mse_loss, perceptual_loss, \n",
    "                                  gan_loss, DEVICE)\n",
    "    \n",
    "    val_loss_dict = validate(generator, discriminator, val_loader, \n",
    "                             mse_loss, perceptual_loss, gan_loss, DEVICE)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses['g'].append(train_loss_dict['g_loss'])\n",
    "    train_losses['d'].append(train_loss_dict['d_loss'])\n",
    "    train_losses['l1'].append(train_loss_dict['l1_loss'])\n",
    "    train_losses['perc'].append(train_loss_dict['perc_loss'])\n",
    "    train_losses['adv'].append(train_loss_dict['adv_loss'])\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"G_Loss: {train_loss_dict['g_loss']:.4f} | \"\n",
    "              f\"D_Loss: {train_loss_dict['d_loss']:.4f} | \"\n",
    "              f\"L1: {train_loss_dict['l1_loss']:.4f} | \"\n",
    "              f\"Perc: {train_loss_dict['perc_loss']:.4f} | \"\n",
    "              f\"Adv: {train_loss_dict['adv_loss']:.4f}\", end=\"\")\n",
    "        \n",
    "        val_g_loss = val_loss_dict['g_loss']\n",
    "        \n",
    "        # Save checkpoint at every epoch\n",
    "        ckpt_path = MODEL_SAVE_DIR / f'unet_gan_checkpoint_{epoch}.pt'\n",
    "        save_checkpoint(generator, discriminator, optimizer_g, optimizer_d, \n",
    "                       epoch, train_losses, val_loss_dict, ckpt_path)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_g_loss < best_val_loss:\n",
    "            best_val_loss = val_g_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best models\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "                'val_loss': val_g_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_loss_dict\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_SAVE_DIR / 'unet_gan_best.pt')\n",
    "            print(\" (Best)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "            \n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "                break\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285cd1d5-59ce-4065-b844-ca048735129d",
   "metadata": {},
   "source": [
    "## Epoch 17 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f802b4-598b-4b83-8068-6c1a15047e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best validation loss from checkpoint: 0.1016\n",
      "\n",
      "Starting training...\n",
      "\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17/20 | G_Loss: 0.0838 | D_Loss: 0.0122 | L1: 0.0571 | Perc: 0.1686 | Adv: 0.9842 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18/20 | G_Loss: 0.0835 | D_Loss: 0.0116 | L1: 0.0568 | Perc: 0.1684 | Adv: 0.9850 (Best)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 12679/18269 [1:13:29<35:05,  2.65it/s, G_loss=0.0430, D_loss=0.0028]  "
     ]
    }
   ],
   "source": [
    "# Initialize training variables (may be restored from checkpoint above)\n",
    "if 'train_losses' not in locals():\n",
    "    train_losses = {'g': [], 'd': [], 'l1': [], 'perc': [], 'adv': []}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Calculate best_val_loss from history if resuming\n",
    "if len(train_losses['g']) > 0:\n",
    "    # Find minimum validation loss from saved history\n",
    "    if 'val_losses_saved' in locals() and val_losses_saved:\n",
    "        best_val_loss = val_losses_saved.get('g_loss', float('inf'))\n",
    "        print(f\"üìä Best validation loss from checkpoint: {best_val_loss:.4f}\\n\")\n",
    "\n",
    "print(f\"Starting training...\\n\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS + 1):\n",
    "    train_loss_dict = train_epoch(generator, discriminator, train_loader, \n",
    "                                  optimizer_g, optimizer_d, mse_loss, perceptual_loss, \n",
    "                                  gan_loss, DEVICE)\n",
    "    \n",
    "    val_loss_dict = validate(generator, discriminator, val_loader, \n",
    "                             mse_loss, perceptual_loss, gan_loss, DEVICE)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses['g'].append(train_loss_dict['g_loss'])\n",
    "    train_losses['d'].append(train_loss_dict['d_loss'])\n",
    "    train_losses['l1'].append(train_loss_dict['l1_loss'])\n",
    "    train_losses['perc'].append(train_loss_dict['perc_loss'])\n",
    "    train_losses['adv'].append(train_loss_dict['adv_loss'])\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "              f\"G_Loss: {train_loss_dict['g_loss']:.4f} | \"\n",
    "              f\"D_Loss: {train_loss_dict['d_loss']:.4f} | \"\n",
    "              f\"L1: {train_loss_dict['l1_loss']:.4f} | \"\n",
    "              f\"Perc: {train_loss_dict['perc_loss']:.4f} | \"\n",
    "              f\"Adv: {train_loss_dict['adv_loss']:.4f}\", end=\"\")\n",
    "        \n",
    "        val_g_loss = val_loss_dict['g_loss']\n",
    "        \n",
    "        # Save checkpoint at every epoch\n",
    "        ckpt_path = MODEL_SAVE_DIR / f'unet_gan_checkpoint_{epoch}.pt'\n",
    "        save_checkpoint(generator, discriminator, optimizer_g, optimizer_d, \n",
    "                       epoch, train_losses, val_loss_dict, ckpt_path)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_g_loss < best_val_loss:\n",
    "            best_val_loss = val_g_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best models\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "                'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "                'val_loss': val_g_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_loss_dict\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_SAVE_DIR / 'unet_gan_best.pt')\n",
    "            print(\" (Best)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
    "            \n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch} epochs\\n\")\n",
    "                break\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e928ef",
   "metadata": {},
   "source": [
    "# Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(generator, test_loader, mse_loss, device):\n",
    "    \"\"\"Evaluate generator on test set\"\"\"\n",
    "    generator.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "        for (pre, post), target in pbar:\n",
    "            inputs = torch.cat([pre, post], dim=1).to(device)\n",
    "            targets = target.to(device)\n",
    "            \n",
    "            outputs = generator(inputs)\n",
    "            loss = mse_loss(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.append(outputs.cpu())\n",
    "            targets_list.append(targets.cpu())\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            del inputs, outputs, targets, loss\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return avg_loss, predictions, targets_list\n",
    "\n",
    "\n",
    "# ========== Load Latest Checkpoint for Testing ==========\n",
    "print(\"üîç Loading latest model checkpoint for testing...\\n\")\n",
    "\n",
    "# First try to load best model\n",
    "best_model_path = MODEL_SAVE_DIR / 'unet_gan_best.pt'\n",
    "checkpoint_to_load = None\n",
    "\n",
    "if best_model_path.exists():\n",
    "    checkpoint_to_load = best_model_path\n",
    "    print(f\"‚úÖ Found best model: {best_model_path.name}\")\n",
    "else:\n",
    "    # If best model not found, try to load latest checkpoint\n",
    "    latest_ckpt_info = get_latest_checkpoint(MODEL_SAVE_DIR, prefix='unet_gan_checkpoint')\n",
    "    if latest_ckpt_info is not None:\n",
    "        latest_ckpt_path, latest_epoch = latest_ckpt_info\n",
    "        checkpoint_to_load = latest_ckpt_path\n",
    "        print(f\"‚úÖ Found latest checkpoint: {latest_ckpt_path.name} (Epoch {latest_epoch})\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No checkpoint found - using current model state\")\n",
    "\n",
    "# Load the checkpoint if found\n",
    "if checkpoint_to_load is not None:\n",
    "    checkpoint = torch.load(checkpoint_to_load, map_location=DEVICE)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    print(f\"‚úÖ Generator loaded from: {checkpoint_to_load.name}\")\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"   Epoch: {checkpoint['epoch']}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\\n\")\n",
    "test_loss, predictions, targets_list = evaluate(generator, test_loader, mse_loss, DEVICE)\n",
    "\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed75644",
   "metadata": {},
   "source": [
    "# Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef24279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all predictions and targets\n",
    "all_predictions = torch.cat(predictions, dim=0)  # (N, 1, H, W)\n",
    "all_targets = torch.cat(targets_list, dim=0)      # (N, 1, H, W)\n",
    "\n",
    "# Visualize some predictions\n",
    "n_samples = 4\n",
    "fig, axes = plt.subplots(n_samples, 3, figsize=(12, 4*n_samples))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    pred = all_predictions[i, 0].numpy()\n",
    "    target = all_targets[i, 0].numpy()\n",
    "    diff = np.abs(pred - target)\n",
    "    \n",
    "    axes[i, 0].imshow(target, cmap='gray')\n",
    "    axes[i, 0].set_title(f'Target Slice {i+1}', fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(pred, cmap='gray')\n",
    "    axes[i, 1].set_title(f'GAN-Generated Slice {i+1}', fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    im = axes[i, 2].imshow(diff, cmap='hot')\n",
    "    axes[i, 2].set_title(f'Difference {i+1}', fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "    plt.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "pred_path = RESULTS_SAVE_DIR / 'unet_gan_predictions.png'\n",
    "plt.savefig(pred_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Predictions saved to {pred_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f45df6",
   "metadata": {},
   "source": [
    "# Compute Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4622e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SSIM and PSNR scores\n",
    "all_predictions_np = all_predictions.numpy()  # (N, 1, H, W)\n",
    "all_targets_np = all_targets.numpy()          # (N, 1, H, W)\n",
    "\n",
    "ssim_scores = []\n",
    "psnr_scores = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING QUALITY METRICS FOR UNet-GAN MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i in range(len(all_predictions_np)):\n",
    "    pred = all_predictions_np[i, 0]  # (H, W)\n",
    "    target = all_targets_np[i, 0]     # (H, W)\n",
    "    \n",
    "    # Calculate SSIM\n",
    "    ssim_score = ssim(target, pred, data_range=pred.max() - pred.min())\n",
    "    ssim_scores.append(ssim_score)\n",
    "    \n",
    "    # Calculate PSNR\n",
    "    psnr_score = psnr(target, pred, data_range=pred.max() - pred.min())\n",
    "    psnr_scores.append(psnr_score)\n",
    "\n",
    "ssim_scores = np.array(ssim_scores)\n",
    "psnr_scores = np.array(psnr_scores)\n",
    "\n",
    "print(f\"\\nüìä UNet-GAN MODEL - IMAGE QUALITY METRICS:\")\n",
    "print(f\"\\n{'Metric':<20} {'Mean':<12} {'Std Dev':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(f\"{'-'*68}\")\n",
    "print(f\"{'SSIM':20} {ssim_scores.mean():<12.4f} {ssim_scores.std():<12.4f} {ssim_scores.min():<12.4f} {ssim_scores.max():<12.4f}\")\n",
    "print(f\"{'PSNR (dB)':20} {psnr_scores.mean():<12.4f} {psnr_scores.std():<12.4f} {psnr_scores.min():<12.4f} {psnr_scores.max():<12.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Per-Sample Metrics (first 10 samples):\")\n",
    "for i in range(min(10, len(ssim_scores))):\n",
    "    print(f\"   Sample {i+1:3d}: SSIM = {ssim_scores[i]:.4f}, PSNR = {psnr_scores[i]:.2f} dB\")\n",
    "\n",
    "if len(ssim_scores) > 10:\n",
    "    print(f\"   ... and {len(ssim_scores)-10} more samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2990c",
   "metadata": {},
   "source": [
    "# Training History & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Generator Loss\n",
    "axes[0, 0].plot(train_losses['g'], label='Train', linewidth=2, marker='o', markersize=3)\n",
    "axes[0, 0].set_title('Generator Loss', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Discriminator Loss\n",
    "axes[0, 1].plot(train_losses['d'], label='Train', linewidth=2, marker='o', markersize=3, color='orange')\n",
    "axes[0, 1].set_title('Discriminator Loss', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# L1 Loss\n",
    "axes[0, 2].plot(train_losses['l1'], label='Train', linewidth=2, marker='o', markersize=3, color='green')\n",
    "axes[0, 2].set_title('L1 (Reconstruction) Loss', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Perceptual Loss\n",
    "axes[1, 0].plot(train_losses['perc'], label='Train', linewidth=2, marker='o', markersize=3, color='red')\n",
    "axes[1, 0].set_title('Perceptual Loss', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Adversarial Loss\n",
    "axes[1, 1].plot(train_losses['adv'], label='Train', linewidth=2, marker='o', markersize=3, color='purple')\n",
    "axes[1, 1].set_title('Adversarial Loss', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Combined Loss\n",
    "axes[1, 2].plot(train_losses['g'], label='Generator', linewidth=2, marker='o', markersize=3)\n",
    "axes[1, 2].plot(train_losses['d'], label='Discriminator', linewidth=2, marker='s', markersize=3)\n",
    "axes[1, 2].set_title('Combined Losses', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Loss')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = RESULTS_SAVE_DIR / 'unet_gan_training_curves.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training curves saved to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867e274-82d4-491e-96ac-ab5bb2e3a145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_losses_saved = checkpoint.get('val_losses', {})\n",
    "best_val_loss = val_losses_saved.get('g_loss', float('inf'))\n",
    "best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses_saved,\n",
    "    'test_loss': test_loss,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'epochs_trained': len(train_losses['g']),\n",
    "    'metrics': {\n",
    "        'ssim_mean': float(ssim_scores.mean()),\n",
    "        'ssim_std': float(ssim_scores.std()),\n",
    "        'psnr_mean': float(psnr_scores.mean()),\n",
    "        'psnr_std': float(psnr_scores.std())\n",
    "    },\n",
    "    'config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate_g': LEARNING_RATE_G,\n",
    "        'learning_rate_d': LEARNING_RATE_D,\n",
    "        'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
    "        'augmentation': AUGMENT,\n",
    "        'generator': 'UNet with skip connections',\n",
    "        'discriminator': 'PatchGAN',\n",
    "        'gan_type': 'Least Squares GAN (LSGAN)',\n",
    "        'loss_weights': {\n",
    "            'lambda_l1': LAMBDA_L1,\n",
    "            'lambda_perceptual': LAMBDA_PERCEPTUAL,\n",
    "            'lambda_adversarial': LAMBDA_ADVERSARIAL\n",
    "        }\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "log_path = RESULTS_SAVE_DIR / 'unet_gan_history.json'\n",
    "with open(log_path, 'w') as f:\n",
    "    json.dump(history, f, indent=4)\n",
    "\n",
    "print(f\"Training history saved to {log_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"UNet-GAN TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"   Generator: UNet with skip connections\")\n",
    "print(f\"   Discriminator: PatchGAN (patch-based)\")\n",
    "print(f\"   GAN Type: Least Squares GAN (LSGAN)\")\n",
    "print(f\"   Epochs trained: {len(train_losses['g'])}\")\n",
    "print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"   Final test loss: {test_loss:.4f}\")\n",
    "print(f\"\\nQuality Metrics:\")\n",
    "print(f\"   SSIM: {ssim_scores.mean():.4f} ¬± {ssim_scores.std():.4f}\")\n",
    "print(f\"   PSNR: {psnr_scores.mean():.4f} ¬± {psnr_scores.std():.4f} dB\")\n",
    "print(f\"\\nLoss Weights:\")\n",
    "print(f\"   L1 (Reconstruction): {LAMBDA_L1}\")\n",
    "print(f\"   Perceptual: {LAMBDA_PERCEPTUAL}\")\n",
    "print(f\"   Adversarial: {LAMBDA_ADVERSARIAL}\")\n",
    "print(f\"\\nFiles saved to: {MODEL_SAVE_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a210574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmia-2",
   "language": "python",
   "name": "dlmia-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
